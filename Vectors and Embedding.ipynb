{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a661f40-c0d7-4d67-bc58-9ca1f62d57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "OPENAI_API_KEY = \"sk-proj-B4fsYjOG4RM3LT15ig1hT3BlbkFJoTg40k62UTmoIP24MqeK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7d9a8c6-1cd8-403b-b3d8-eb24ca610d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"test1.pdf\"), \n",
    "    PyPDFLoader(\"test2.pdf\"), \n",
    "    PyPDFLoader(\"test3.pdf\"), \n",
    "    PyPDFLoader(\"test4.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f54dea-75d2-4a19-ab53-a2bfee828571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4179b2f2-14f0-402b-9af3-8a5454861105",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c60def32-0adc-431f-a047-b855edf1f170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \\nof the class, and then we'll start to  talk a bit about machine learning.  \\nBy way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \\nI personally work in machine learning, and I' ve worked on it for about 15 years now, and\", metadata={'source': 'test1.pdf', 'page': 0}),\n",
       " Document(page_content=\"I actually think that machine learning is th e most exciting field of all the computer \\nsciences. So I'm actually always excited about  teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thin g in computer science, but \\nthe most exciting thing in all of human e ndeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or\", metadata={'source': 'test1.pdf', 'page': 0}),\n",
       " Document(page_content=\"related to the machine learni ng and all aspects of machin e learning. Paul Baumstarck \\nworks in machine learning and computer vision.  Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computa tional biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is  the head TA — he's head TA two years\", metadata={'source': 'test1.pdf', 'page': 0}),\n",
       " Document(page_content=\"in a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he's not here  — Daniel applies l earning algorithms to \\nproblems in natural language processing.  \\nSo you'll get to know the TAs and me much be tter throughout this quarter, but just from \\nthe sorts of things the TA's do, I hope you can  already tell that machine learning is a \\nhighly interdisciplinary topic in which just the TAs find l earning algorithms to problems\", metadata={'source': 'test1.pdf', 'page': 0}),\n",
       " Document(page_content='in computer vision and biology and robots a nd language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in  computer systems or databases to economists \\nand sort of also an unending stream of  people from industry coming to Stanford \\ninterested in applying machine learni ng methods to their own problems.', metadata={'source': 'test1.pdf', 'page': 0}),\n",
       " Document(page_content='So yeah, this is fun. A couple of weeks ago, a student actually forwar ded to me an article \\nin \"Computer World\" about the 12 IT skills th at employers can\\'t say no to. So it\\'s about \\nsort of the 12 most desirabl e skills in all of IT and all of information technology, and \\ntopping the list was actually machine lear ning. So I think this is a good time to be \\nlearning this stuff and learning algorithms and having a large impact on many segments \\nof science and industry.', metadata={'source': 'test1.pdf', 'page': 0}),\n",
       " Document(page_content=\"I'm actually curious about something. Learni ng algorithms is one of the things that \\ntouches many areas of science and industrie s, and I'm just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from  EE? Oh, okay, maybe about a fifth. How\", metadata={'source': 'test1.pdf', 'page': 0}),\n",
       " Document(page_content=\"many biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \\nstatistics? Okay, a few. So where are the rest of you from?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : Say again?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : iCME. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Civi and what else?  \\nStudent : [Inaudible]  \\nInstructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \\nStudent : Chemi.  \\nInstructor (Andrew Ng) : Chemi. Cool.\", metadata={'source': 'test1.pdf', 'page': 1}),\n",
       " Document(page_content='Student : [Inaudible].  \\nInstructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon? MSNE. All ri ght. Cool. Yeah.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Endo —  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can', metadata={'source': 'test1.pdf', 'page': 1}),\n",
       " Document(page_content=\"tell from a cross-section of th is class, I think we're a very diverse audience in this room, \\nand that's one of the things that makes this class fun to teach and fun to be in, I think.\", metadata={'source': 'test1.pdf', 'page': 1}),\n",
       " Document(page_content=\"So in this class, we've tried to convey to you a broad set of principl es and tools that will \\nbe useful for doing many, many things. And ev ery time I teach this class, I can actually \\nvery confidently say that af ter December, no matter what yo u're going to do after this \\nDecember when you've sort of completed this  class, you'll find the things you learn in \\nthis class very useful, and these things will be useful pretty much no matter what you end \\nup doing later in your life.\", metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content=\"So I have more logistics to go over later, but let's say a few more words about machine \\nlearning. I feel that machine learning grew out of  early work in AI, early work in artificial \\nintelligence. And over the last — I wanna say last 15 or last 20 years or so, it's been viewed as a sort of growing new capability for computers. And in particular, it turns out \\nthat there are many programs or there are many applications that you can't program by \\nhand.\", metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content='hand.  \\nFor example, if you want to get a computer to read handwritten characters, to read sort of \\nhandwritten digits, that actual ly turns out to be amazingly difficult to write a piece of \\nsoftware to take this input, an image of some thing that I wrote and to  figure out just what \\nit is, to translate my cursive handwriting into — to extract the characters I wrote out in \\nlonghand. And other things: One thing that my students and I do is autonomous flight. It', metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content='turns out to be extremely difficult to sit dow n and write a program to  fly a helicopter.  \\nBut in contrast, if you want to do things like to get software to fl y a helicopter or have \\nsoftware recognize handwritten digits, one very  successful approach is to use a learning \\nalgorithm and have a computer learn by its elf how to, say, recognize your handwriting. \\nAnd in fact, handwritten digit recognition, this is pretty much the only approach that', metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content=\"works well. It uses applications that are hard to program by hand.  \\nLearning algorithms has also made I guess sign ificant inroads in what's sometimes called \\ndatabase mining. So, for example, with the growth of IT and computers, increasingly \\nmany hospitals are keeping around medical reco rds of what sort of patients, what \\nproblems they had, what their prognoses was,  what the outcome was. And taking all of\", metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content=\"these medical records, which started to be digitized only about maybe 15 years, applying \\nlearning algorithms to them can turn raw medi cal records into what I might loosely call \\nmedical knowledge in which we start to detect trends in medical practice and even start to \\nalter medical practice as a result of me dical knowledge that's derived by applying \\nlearning algorithms to the sorts of medical r ecords that hospitals have just been building\", metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content=\"over the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think \\nhalf a dozen times a day or maybe a dozen  times a day or more, and often without \\nknowing it. So, for example, every time you se nd mail via the US Postal System, turns \\nout there's an algorithm that tries to automa tically read the zip code you wrote on your \\nenvelope, and that's done by a learning al gorithm. So every time you send US mail, you\", metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content='are using a learning algorithm, perhap s without even being aware of it.', metadata={'source': 'test1.pdf', 'page': 2}),\n",
       " Document(page_content=\"Similarly, every time you write a check, I ac tually don't know the number for this, but a \\nsignificant fraction of checks that you write are processed by a learning algorithm that's \\nlearned to read the digits, so the dolla r amount that you wrote down on your check. So \\nevery time you write a check, there's anot her learning algorithm that you're probably \\nusing without even being aware of it.  \\nIf you use a credit card, or I know at least one phone compan y was doing this, and lots of\", metadata={'source': 'test1.pdf', 'page': 3}),\n",
       " Document(page_content=\"companies like eBay as well that do electr onic transactions, there's a good chance that \\nthere's a learning algorithm in the backgr ound trying to figure out if, say, your credit \\ncard's been stolen or if someone's engaging in a fraudulent transaction.  \\nIf you use a website like Amazon or Netflix that will often recommend books for you to \\nbuy or movies for you to rent or whatever , these are other examples of learning\", metadata={'source': 'test1.pdf', 'page': 3}),\n",
       " Document(page_content=\"algorithms that have learned what sorts of th ings you like to buy or what sorts of movies \\nyou like to watch and can therefore give  customized recommendations to you.  \\nJust about a week ago, I had my car serviced, and even there, my car mechanic was trying \\nto explain to me some learning algorithm in th e innards of my car th at's sort of doing its \\nbest to optimize my driving performan ce for fuel efficiency or something.\", metadata={'source': 'test1.pdf', 'page': 3}),\n",
       " Document(page_content=\"So, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.  \\nAnd of course, learning algorithms are also  doing things like giving us a growing \\nunderstanding of the human genome. So if so meday we ever find a cure for cancer, I bet \\nlearning algorithms will have had a large role in that. That's sort of the thing that Tom \\nworks on, yes?  \\nSo in teaching this class, I sort of have thre e goals. One of them is just to I hope convey\", metadata={'source': 'test1.pdf', 'page': 3}),\n",
       " Document(page_content=\"some of my own excitement a bout machine learning to you.  \\nThe second goal is by the end of this class, I hope all of you will be ab le to apply state-of-\\nthe-art machine learning algorithms to whatev er problems you're interested in. And if you \\never need to build a system for reading zi p codes, you'll know how to do that by the end \\nof this class.  \\nAnd lastly, by the end of this class, I reali ze that only a subset of  you are interested in\", metadata={'source': 'test1.pdf', 'page': 3}),\n",
       " Document(page_content=\"doing research in machine learning, but by the c onclusion of this class,  I hope that all of \\nyou will actually be well qualified to star t doing research in machine learning, okay?  \\nSo let's say a few words about logistics. The prerequisites of this class are written on one \\nof the handouts, are as follows: In this class, I'm going to assume that all of you have sort \\nof basic knowledge of computer science and kn owledge of the basic computer skills and\", metadata={'source': 'test1.pdf', 'page': 3}),\n",
       " Document(page_content='principles. So I assume all of you know what big?O notation, that all of you know about \\nsort of data structures like  queues, stacks, binary trees , and that all of you know enough \\nprogramming skills to, like, write a simple co mputer program. And it turns out that most', metadata={'source': 'test1.pdf', 'page': 3}),\n",
       " Document(page_content=\"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation\", metadata={'source': 'test1.pdf', 'page': 4}),\n",
       " Document(page_content=\"is, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate\", metadata={'source': 'test1.pdf', 'page': 4}),\n",
       " Document(page_content=\"linear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better.\", metadata={'source': 'test1.pdf', 'page': 4}),\n",
       " Document(page_content=\"But if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.  \\nSo there are a couple more logisti cal things I should deal with in  this class. One is that, as \\nmost of you know, CS229 is a televised cla ss. And in fact, I guess many of you are \\nprobably watching this at home on TV, so I' m gonna say hi to our home viewers.  \\nSo earlier this year, I appro ached SCPD, which televises th ese classes, about trying to\", metadata={'source': 'test1.pdf', 'page': 4}),\n",
       " Document(page_content=\"make a small number of Stanford classes publ icly available or posting the videos on the \\nweb. And so this year, Stanford is actually starting a small pilot program in which we'll \\npost videos of a small number of classes onlin e, so on the Internet in a way that makes it \\npublicly accessible to everyone. I'm very exc ited about that because machine learning in \\nschool, let's get the word out there.  \\nOne of the consequences of this is that — let's see — so videos  or pictures of the students\", metadata={'source': 'test1.pdf', 'page': 4}),\n",
       " Document(page_content=\"in this classroom will not be posted online, so your images — so don't worry about being \\nby seeing your own face appear on YouTube one day. But the microphones may pick up your voices, so I guess the consequence of that is that because microphones may pick up your voices, no matter how irritated you are at  me, don't yell out swear words in the \\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?\", metadata={'source': 'test1.pdf', 'page': 4}),\n",
       " Document(page_content=\"Let's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\", metadata={'source': 'test1.pdf', 'page': 4}),\n",
       " Document(page_content=\"So all right, online resources. The class has a home page, so it's in on the handouts. I \\nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \\nhomework assignments or things like that, we  usually won't sort of — in the mission of \\nsaving trees, we will usually not give out many handouts in class. So homework \\nassignments, homework solutions will be posted online at the course home page.\", metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content=\"As far as this class, I've also written, a nd I guess I've also revised every year a set of \\nfairly detailed lecture notes that cover the te chnical content of this  class. And so if you \\nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \\nall the math and equations and so on  that I'll be doing in class.  \\nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a\", metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content=\"newsgroup that's sort of a forum for people in  the class to get to  know each other and \\nhave whatever discussions you want to ha ve amongst yourselves. So the class newsgroup \\nwill not be monitored by the TAs and me. But this is a place for you to form study groups \\nor find project partners or discuss homework problems and so on, and it's not monitored \\nby the TAs and me. So feel free to ta lk trash about this class there.\", metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content=\"If you want to contact the teaching staff, pl ease use the email address written down here, \\ncs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework probl ems, please say in the subject line which\", metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content=\"assignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me  appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thi ng that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this cla ss more is if you form a study \\ngroup.\", metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content=\"group.  \\nSo start looking around where you' re sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also  post on the class news group if you want to \\nuse that to try to form a study group.\", metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content=\"But some of the problems sets in this cla ss are reasonably difficult.  People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a be tter learning experience if you form a \\nstudy group of people to work with. So I definitely encourage you to do that.  \\nAnd just to say a word on the honor code, whic h is I definitely en courage you to form a\", metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content='study group and work together, discuss homew ork problems together. But if you discuss', metadata={'source': 'test1.pdf', 'page': 5}),\n",
       " Document(page_content=\"homework problems with other students, then  I'll ask you to sort of go home and write \\ndown your own solutions independe ntly without referring to note s that were taken in any \\nof your joint study sessions.  \\nSo in other words, when you turn in a hom ework problem, what you turn in should be \\nsomething that was reconstructed independe ntly by yourself and w ithout referring to \\nnotes that you took during your  study sessions with other people, okay? And obviously,\", metadata={'source': 'test1.pdf', 'page': 6}),\n",
       " Document(page_content=\"showing your solutions to othe rs or copying other solutions  directly is right out.  \\nWe occasionally also reuse problem set questions from previous years so that the \\nproblems are a bit more debugged and work more  smoothly. And as a result of that, I also \\nask you not to look at solutions from previous ye ars, and this includes both sort of official \\nsolutions that we've given out to previous gene rations of this class and previous solutions\", metadata={'source': 'test1.pdf', 'page': 6}),\n",
       " Document(page_content=\"that people that have taken this class in previous years may have written out by \\nthemselves, okay?  \\nSadly, in this class, there are usually — sadly, in previous y ears, there have often been a \\nfew honor code violations in this class. And last year, I think I pr osecuted five honor code \\nviolations, which I think is a ridiculously large number. And so just don't work without \\nsolutions, and hopefully there'll be zero honor code  violations this year. I'd love for that \\nto happen.\", metadata={'source': 'test1.pdf', 'page': 6}),\n",
       " Document(page_content=\"to happen.  \\nThe section here on the late homework polic y if you ever want to hand in a homework \\nlate, I'll leave you to r ead that yourself.  \\nWe also have a midterm, which is scheduled for Thursday, 8th of November at 6:00 p.m., \\nso please keep that evening free.  \\nAnd let's see. And one more administrative thing I wanted to sa y is about the class \\nproject. So part of the goal of this cla ss is to leave you well eq uipped to apply machine\", metadata={'source': 'test1.pdf', 'page': 6}),\n",
       " Document(page_content=\"learning algorithms to a problem or to do rese arch in machine learning. And so as part of \\nthis class, I'll ask you to execute a small resear ch project sort of as a small term project.  \\nAnd what most students do for this is either  apply machine learning to a problem that you \\nfind interesting or investigate some aspect of  machine learning. So to those of you that \\nare either already doing research or to those of you who are in industry, you're taking this\", metadata={'source': 'test1.pdf', 'page': 6}),\n",
       " Document(page_content=\"from a company, one fantastic sort of way to do a class project would be if you apply \\nmachine learning algorithms to a problem that  you're interested in, to a problem that \\nyou're already working on, whether it be a scien ce research problem or sort of a problem \\nin industry where you're trying to get a syst em to work using a learning algorithm.  \\nTo those of you that are not currently doing re search, one great way to do a project would\", metadata={'source': 'test1.pdf', 'page': 6}),\n",
       " Document(page_content='be if you apply learning algorithms to just pick a problem that you care about. Pick a \\nproblem that you find interesting, and apply lear ning algorithms to that  and play with the \\nideas and see what happens.', metadata={'source': 'test1.pdf', 'page': 6}),\n",
       " Document(page_content=\"And let's see. Oh, and the goal of the projec t should really be for you to do a publishable \\npiece of research in machine learning, okay?  \\nAnd if you go to the course website, you'll actuall y find a list of the projects that students \\nhad done last year. And so I'm holding the li st in my hand. You can  go home later and \\ntake a look at it online.  \\nBut reading down this list, I see that last year, there were st udents that ap plied learning\", metadata={'source': 'test1.pdf', 'page': 7}),\n",
       " Document(page_content=\"algorithms to control a snake robot. Ther e was a few projects on improving learning \\nalgorithms. There's a project on flying autonomous  aircraft. There was a project actually \\ndone by our TA Paul on improvi ng computer vision algorithms  using machine learning.  \\nThere are a couple of project s on Netflix rankings using learning algorithms; a few \\nmedical robots; ones on segmenting [inaudibl e] to segmenting pieces of the body using\", metadata={'source': 'test1.pdf', 'page': 7}),\n",
       " Document(page_content='learning algorithms; one on musical instrume nt detection; anot her on irony sequence \\nalignment; and a few algorithms on understandin g the brain neuroscience, actually quite a \\nfew projects on neuroscience; a couple of projects on unde scending fMRI data on brain \\nscans, and so on; another project on market makings, the financial trading. There was an \\ninteresting project on trying to use learning algorithms to decide what is it that makes a', metadata={'source': 'test1.pdf', 'page': 7}),\n",
       " Document(page_content=\"person's face physically attractive. There's a learning algorithm on op tical illusions, and \\nso on.  \\nAnd it goes on, so lots of fun projects. A nd take a look, then come up with your own \\nideas. But whatever you find cool and interest ing, I hope you'll be able to make machine \\nlearning a project out of it. Yeah, question?  \\nStudent : Are these gro up projects?  \\nInstructor (Andrew Ng): Oh, yes, thank you.  \\nStudent : So how many people can be in a group?\", metadata={'source': 'test1.pdf', 'page': 7}),\n",
       " Document(page_content='Instructor (Andrew Ng): Right. So projects can be done in  groups of up to three people. \\nSo as part of forming study groups, later t oday as you get to know your classmates, I \\ndefinitely also encourage you to grab two ot her people and form a group of up to three \\npeople for your project, okay? And just start brainstorming ideas for now amongst \\nyourselves. You can also come and talk to me or the TAs if you want to brainstorm ideas \\nwith us.', metadata={'source': 'test1.pdf', 'page': 7}),\n",
       " Document(page_content=\"with us.  \\nOkay. So one more organizational ques tion. I'm curious, how many of you know \\nMATLAB? Wow, cool, quite a lot. Okay. So as part of the — act ually how many of you \\nknow Octave or have used Octave ? Oh, okay, much smaller number.  \\nSo as part of this class, especially in the homeworks, we'll ask you to implement a few \\nprograms, a few machine learning algorithms as  part of the homeworks. And most of\", metadata={'source': 'test1.pdf', 'page': 7}),\n",
       " Document(page_content=\"those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn't.  \\nSo I guess for those of you that haven't s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to\", metadata={'source': 'test1.pdf', 'page': 8}),\n",
       " Document(page_content=\"plot data. And it's sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon't have a MATLAB license, for the purposes of  this class, there's also — [inaudible] \\nwrite that down [inaudible] MATLAB — there' s also a software package called Octave\", metadata={'source': 'test1.pdf', 'page': 8}),\n",
       " Document(page_content=\"that you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it's free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven't seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He's taught it for many years.\", metadata={'source': 'test1.pdf', 'page': 8}),\n",
       " Document(page_content='So one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this', metadata={'source': 'test1.pdf', 'page': 8}),\n",
       " Document(page_content='machine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.', metadata={'source': 'test1.pdf', 'page': 8}),\n",
       " Document(page_content=\"Okay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they'll also be recorded and televi sed. And we'll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we'll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven't seen probability or\", metadata={'source': 'test1.pdf', 'page': 8}),\n",
       " Document(page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\", metadata={'source': 'test1.pdf', 'page': 8}),\n",
       " Document(page_content=\"So later this quarter, we'll use the discussion sections to talk about things like convex \\noptimization, to talk a little bit about hidde n Markov models, which is a type of machine \\nlearning algorithm for modeling time series and a few other things, so  extensions to the \\nmaterials that I'll be covering in the main  lectures. And attend ance at the discussion \\nsections is optional, okay?  \\nSo that was all I had from l ogistics. Before we move on to start talking a bit about\", metadata={'source': 'test1.pdf', 'page': 9}),\n",
       " Document(page_content=\"machine learning, let me check what questions you have. Yeah?  \\nStudent : [Inaudible] R or something like that?  \\nInstructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \\nwelcome to use R, but I would strongly advi se against it, mainly because in the last \\nproblem set, we actually supply some code th at will run in Octave  but that would be \\nsomewhat painful for you to translate into R yourself. So for your other assignments, if\", metadata={'source': 'test1.pdf', 'page': 9}),\n",
       " Document(page_content=\"you wanna submit a solution in R, that's fi ne. But I think MATLAB is actually totally \\nworth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \\nbit more often for various reasons. Yeah?  \\nStudent : For the [inaudible] pr oject [inaudible]?  \\nInstructor (Andrew Ng) : So for the term project, you're welcome to do it in smaller \\ngroups of three, or you're welcome to do it by yo urself or in groups of two. Grading is the\", metadata={'source': 'test1.pdf', 'page': 9}),\n",
       " Document(page_content=\"same regardless of the group size, so with  a larger group, you probably — I recommend \\ntrying to form a team, but it's actually totally fine to do it in a sma ller group if you want.  \\nStudent : [Inaudible] what language [inaudible]?  \\nInstructor (Andrew Ng): So let's see. There is no C programming in this class other \\nthan any that you may choose to do yourself in your project. So all the homeworks can be\", metadata={'source': 'test1.pdf', 'page': 9}),\n",
       " Document(page_content=\"done in MATLAB or Octave, and let's see. A nd I guess the program prerequisites is more \\nthe ability to understand big?O notation and know ledge of what a data structure, like a \\nlinked list or a queue or bina ry treatments, more so than  your knowledge of C or Java \\nspecifically. Yeah?  \\nStudent : Looking at the end semester project, I mean, what exactly will you be testing \\nover there? [Inaudible]?  \\nInstructor (Andrew Ng) : Of the project?  \\nStudent : Yeah.\", metadata={'source': 'test1.pdf', 'page': 9}),\n",
       " Document(page_content='Student : Yeah.  \\nInstructor (Andrew Ng) : Yeah, let me answer that later.  In a couple of weeks, I shall \\ngive out a handout with guidelines for the pr oject. But for now, we should think of the \\ngoal as being to do a cool piec e of machine learning work that  will let you experience the', metadata={'source': 'test1.pdf', 'page': 9}),\n",
       " Document(page_content=\"joys of machine learning firs thand and really try to think about doing a publishable piece \\nof work.  \\nSo many students will try to build a cool machine learning application. That's probably \\nthe most common project. Some students will try to improve state-of-the-art machine \\nlearning. Some of those projects are also very  successful. It's a littl e bit harder to do. And \\nthere's also a smaller minority of students th at will sometimes try to prove — develop the\", metadata={'source': 'test1.pdf', 'page': 10}),\n",
       " Document(page_content=\"theory of machine learning further or try to  prove theorems about machine learning. So \\nthey're usually great projects of all of those types with applications and machine learning \\nbeing the most common. Anything else? Okay, cool.  \\nSo that was it for logistics. Let's talk about  learning algorithms. So can I have the laptop \\ndisplay, please, or the projector? Actually, co uld you lower the big sc reen? Cool. This is\", metadata={'source': 'test1.pdf', 'page': 10}),\n",
       " Document(page_content=\"amazing customer service. Thank you. I see. Okay, cool. Okay. No, that's fine. I see. \\nOkay. That's cool. Thanks. Okay.  \\nBig screen isn't working toda y, but I hope you can read things  on the smaller screens out \\nthere. Actually, [inaudible] I think this room just got a new projector that — someone \\nsent you an excited email — was it just on Frid ay? — saying we just got a new projector \\nand they said 4,000-to-1 something or othe r brightness ratio. I don't know. Someone was\", metadata={'source': 'test1.pdf', 'page': 10}),\n",
       " Document(page_content=\"very excited about the new projector in this room, but I guess we'll see that in operation \\non Wednesday.  \\nSo start by talking about what machine learni ng is. What is machine learning? Actually, \\ncan you read the text out there? Raise your hand if the text on the small screens is legible. \\nOh, okay, cool, mostly legible. Okay. So I'll just read it out.  \\nSo what is machine learning? Way back in  about 1959, Arthur Samuel defined machine\", metadata={'source': 'test1.pdf', 'page': 10}),\n",
       " Document(page_content='learning informally as the [inaudible] that gives computers to learn — [inaudible] that \\ngives computers the ability to learn without  being explicitly programmed. So Arthur \\nSamuel, so way back in the history of m achine learning, actually did something very \\ncool, which was he wrote a checkers progr am, which would play games of checkers \\nagainst itself.  \\nAnd so because a computer can play thousands  of games against itself relatively quickly,', metadata={'source': 'test1.pdf', 'page': 10}),\n",
       " Document(page_content='Arthur Samuel had his program play thousands  of games against itself, and over time it \\nwould start to learn to rec ognize patterns which led to wi ns and patterns which led to \\nlosses. So over time it learned things like that , \"Gee, if I get a lot of pieces taken by the \\nopponent, then I\\'m more likely to lose than win,\" or, \"Gee, if I get my pieces into a \\ncertain position, then I\\'m especially li kely to win rather than lose.\"', metadata={'source': 'test1.pdf', 'page': 10}),\n",
       " Document(page_content='And so over time, Arthur Samuel had a check ers program that woul d actually learn to \\nplay checkers by learning what are the sort of  board positions that tend to be associated \\nwith wins and what are the boa rd positions that tend to be associated with losses. And \\nway back around 1959, the amazing thing about this was that his program actually \\nlearned to play checkers much better than Arthur Samuel  himself could.', metadata={'source': 'test1.pdf', 'page': 10}),\n",
       " Document(page_content=\"So even today, there are some people that say, well, computers can't do anything that \\nthey're not explicitly programmed to. And Ar thur Samuel's checkers program was maybe \\nthe first I think really convi ncing refutation of this clai m. Namely, Arthur Samuel \\nmanaged to write a checkers program that could play checkers much better than he \\npersonally could, and this is an instance of maybe computers learning to do things that \\nthey were not programmed explicitly to do.\", metadata={'source': 'test1.pdf', 'page': 11}),\n",
       " Document(page_content=\"Here's a more recent, a more modern, more formal definition of machine learning due to \\nTom Mitchell, who says that a well-posed le arning problem is defined as follows: He \\nsays that a computer program is set to lear n from an experience E with respect to some \\ntask T and some performance measure P if  its performance on T as measured by P \\nimproves with experience E. Okay. So not  only is it a definition, it even rhymes.\", metadata={'source': 'test1.pdf', 'page': 11}),\n",
       " Document(page_content=\"So, for example, in the case of checkers, th e experience E that a program has would be \\nthe experience of playing lots of games of checkers against itself, say. The task T is the \\ntask of playing checkers, a nd the performance measure P will be something like the \\nfraction of games it wins against a cert ain set of human opponents. And by this \\ndefinition, we'll say that Arthur Samuel's ch eckers program has learned to play checkers, \\nokay?\", metadata={'source': 'test1.pdf', 'page': 11}),\n",
       " Document(page_content=\"okay?  \\nSo as an overview of what we're going to do in this class, this class is sort of organized \\ninto four major sections. We're gonna talk about four major topics in this class, the first \\nof which is supervised learning. So le t me give you an example of that.  \\nSo suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \\nactually collected a data set for me last week to use in the example later. But suppose that\", metadata={'source': 'test1.pdf', 'page': 11}),\n",
       " Document(page_content=\"you go to collect statistics about how much hous es cost in a certain geographic area. And \\nDan, the TA, collected data from housing pr ices in Portland, Oregon. So what you can do \\nis let's say plot the square footage of the house against the list price of  the house, right, so \\nyou collect data on a bunch of houses. And let' s say you get a data set like this with \\nhouses of different sizes that are li sted for different amounts of money.\", metadata={'source': 'test1.pdf', 'page': 11}),\n",
       " Document(page_content=\"Now, let's say that I'm trying to sell a hous e in the same area as Portland, Oregon as \\nwhere the data comes from. Let's say I have a hou se that's this size in square footage, and \\nI want an algorithm to tell me about how much should I expect my house to sell for. So there are lots of ways to do this, and some of you may have seen elements of what I'm \\nabout to say before.  \\nSo one thing you could do is look at this data and maybe put a straight  line to it. And then\", metadata={'source': 'test1.pdf', 'page': 11}),\n",
       " Document(page_content=\"if this is my house, you may then look at th e straight line and predict that my house is \\ngonna go for about that much money, right? Ther e are other decisions that we can make, \\nwhich we'll talk about later, which is, well, what if I don' t wanna put a straight line? \\nMaybe I should put a quadratic function to it. Ma ybe that fits the data a little bit better. \\nYou notice if you do that, the price of my house goes up a bit, so that'd be nice.\", metadata={'source': 'test1.pdf', 'page': 11}),\n",
       " Document(page_content=\"And this sort of learning pr oblem of learning to predict hous ing prices is an example of \\nwhat's called a supervised learning problem. And the reason that it's called supervised \\nlearning is because we're providing the al gorithm a data set of a bunch of square \\nfootages, a bunch of housing sizes, and as well as sort of the right answer of what the \\nactual prices of a number  of houses were, right?  \\nSo we call this supervised learning because we're supervising the algorithm or, in other\", metadata={'source': 'test1.pdf', 'page': 12}),\n",
       " Document(page_content=\"words, we're giving the algorithm the, quote,  right answer for a number of houses. And \\nthen we want the algorithm to learn the a ssociation between the inputs and the outputs \\nand to sort of give us more of the right answers, okay?  \\nIt turns out this specific exam ple that I drew here is an example of something called a \\nregression problem. And the term regression sort of refers to the fact that the variable \\nyou're trying to predict is a continuous value and price.\", metadata={'source': 'test1.pdf', 'page': 12}),\n",
       " Document(page_content=\"There's another class of supervised learning problems which we'll talk about, which are \\nclassification problems. And so, in a classifi cation problem, the variab le you're trying to \\npredict is discreet rather than continuous . So as one specific example — so actually a \\nstandard data set you can download online [i naudible] that lots of machine learning \\npeople have played with. Let's say you collect  a data set on breast cancer tumors, and you\", metadata={'source': 'test1.pdf', 'page': 12}),\n",
       " Document(page_content=\"want to learn the algorithm to predict wh ether or not a certai n tumor is malignant. \\nMalignant is the opposite of benign, right, so ma lignancy is a sort of harmful, bad tumor. \\nSo we collect some number of features, some  number of properties of these tumors, and \\nfor the sake of sort of having a simple [inaudi ble] explanation, let's just say that we're \\ngoing to look at the size of the tumor and depe nding on the size of the tumor, we'll try to\", metadata={'source': 'test1.pdf', 'page': 12}),\n",
       " Document(page_content=\"figure out whether or not the tu mor is malignant or benign.  \\nSo the tumor is either malignant or benign, and so  the variable in the Y axis is either zero \\nor 1, and so your data set ma y look something like that, righ t? And that's 1 and that's \\nzero, okay? And so this is an example of a classification problem where the variable \\nyou're trying to predict is a discreet value. It 's either zero or 1.  \\nAnd in fact, more generally, there will be many learning problems where we'll have more\", metadata={'source': 'test1.pdf', 'page': 12}),\n",
       " Document(page_content='than one input variable, more than one input f eature and use more than one variable to try \\nto predict, say, whether a tumor is malignant  or benign. So, for example, continuing with \\nthis, you may instead have a data  set that looks like this. I\\'m gonna part this data set in a \\nslightly different way now. And I\\'m making this  data set look much cleaner than it really \\nis in reality for illustration, okay?  \\nFor example, maybe the crosses indicate ma lignant tumors and the \"O\"s may indicate', metadata={'source': 'test1.pdf', 'page': 12}),\n",
       " Document(page_content='benign tumors. And so you may have a data se t comprising patients of  different ages and \\nwho have different tumor sizes and where a cross indicates a mali gnant tumor, and an \\n\"O\" indicates a benign tumor. And you may want  an algorithm to learn to predict, given a \\nnew patient, whether their tumo r is malignant or benign.', metadata={'source': 'test1.pdf', 'page': 12}),\n",
       " Document(page_content=\"So, for example, what a learning algorithm ma y do is maybe come in and decide that a \\nstraight line like that separates the two classes of tumors really well, and so if you have a \\nnew patient who's age and tumor size fall over there, then the algorithm may predict that \\nthe tumor is benign rather than malignant, oka y? So this is just another example of \\nanother supervised learning problem and another classification problem.\", metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content=\"And so it turns out that one of the issues we' ll talk about later in this class is in this \\nspecific example, we're going to try to predic t whether a tumor is malignant or benign \\nbased on two features or based on two inputs, namely the age of the patient and the tumor \\nsize. It turns out that when you look at a real  data set, you find th at learning algorithms \\noften use other sets of features . In the breast cancer data ex ample, you also use properties\", metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content=\"of the tumors, like clump thic kness, uniformity of cell size, uniformity of cell shape, \\n[inaudible] adhesion and so on, so va rious other medical properties.  \\nAnd one of the most interesting things we'll ta lk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or th ree-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite\", metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content=\"dimensional space? Our plots here are two-dime nsional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensi onal space and then does classification using not \\ntwo features like I've done  here, but an infinite number of features.\", metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content=\"And that will actually be one of the most fa scinating things we talk about when we go \\ndeeply into classification al gorithms. And it's actually an in teresting question, right, so \\nthink about how do you even represent an in finite dimensional vector in computer \\nmemory? You don't have an infinite amount of computers. How do you even represent a \\npoint that lies in an infinite dimensional sp ace? We'll talk about that when we get to \\nsupport vector machines, okay?\", metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content=\"So let's see. So that was supervised learning. The second of the four major topics of this \\nclass will be learning theory. So I have a friend who teaches math at a different \\nuniversity, not at Stanford, and when you talk to  him about his work and what he's really \\nout to do, this friend of mine will — he's a ma th professor, right? — this friend of mine \\nwill sort of get the look of wonder in his eyes, and he'll tell you about how in his\", metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content=\"mathematical work, he feels like he's disc overing truth and beauty in the universe. And \\nhe says it in sort of a real ly touching, sincere way, and then  he has this — you can see it \\nin his eyes — he has this deep appreciation of the truth and beauty  in the universe as \\nrevealed to him by the math he does.  \\nIn this class, I'm not gonna do any truth and beauty. In this class,  I'm gonna talk about \\nlearning theory to try to convey to y ou an understanding of how and why learning\", metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content='algorithms work so that we can apply these lear ning algorithms as effectively as possible.  \\nSo, for example, it turns out you can prove su rprisingly deep theorems on when you can \\nguarantee that a learning algorithm will wo rk, all right? So think about a learning', metadata={'source': 'test1.pdf', 'page': 13}),\n",
       " Document(page_content=\"algorithm for reading zip codes. When can  you prove a theorem guaranteeing that a \\nlearning algorithm will be at least 99.9 per cent accurate on reading zip codes? This is \\nactually somewhat surprising. We actually prove theorems showing when you can expect \\nthat to hold.  \\nWe'll also sort of delve into learning theo ry to try to understand what algorithms can \\napproximate different functions well and also  try to understand things like how much\", metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content=\"training data do you need? So how many exampl es of houses do I need in order for your \\nlearning algorithm to recognize the pattern be tween the square footage of a house and its \\nhousing price? And this will help  us answer questions like if you're trying to design a \\nlearning algorithm, should you be spending more time collecting more data or is it a case \\nthat you already have enough data; it would be  a waste of time to try to collect more. \\nOkay?\", metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content=\"Okay?  \\nSo I think learning algorithms are a very powerful tool that  as I walk around sort of \\nindustry in Silicon Valley or as I work with  various businesses in CS and outside CS, I \\nfind that there's often a huge differen ce between how well so meone who really \\nunderstands this stuff can appl y a learning algorithm versus so meone who sort of gets it \\nbut sort of doesn't.  \\nThe analogy I like to think of  is imagine you were going to a carpentry school instead of\", metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content=\"a machine learning class, right? If you go to a carpentry school, they can give you the \\ntools of carpentry. They'll give you a hamme r, a bunch of nails, a screwdriver or \\nwhatever. But a master carpenter will be able to  use those tools far better than most of us \\nin this room. I know a carpen ter can do things with a hammer and nail that I couldn't \\npossibly. And it's actually a littl e bit like that in machine learning, too. One thing that's\", metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content='sadly not taught in many courses on machine l earning is how to take the tools of machine \\nlearning and really, really apply them well.  \\nSo in the same way, so the tools of machin e learning are I wanna say quite a bit more \\nadvanced than the tools of carpentry. Maybe a carpenter will disagree . But a large part of \\nthis class will be just givi ng you the raw tools of machine learning, just the algorithms', metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content=\"and so on. But what I plan to do throughout this entire quarter, not just in the segment of \\nlearning theory, but actually as a theme r unning through everything I do this quarter, will \\nbe to try to convey to you the skills to real ly take the learning al gorithm ideas and really \\nto get them to work on a problem.  \\nIt's sort of hard for me to stand here and say how big a deal that is, but when I walk \\naround companies in Silicon Valley, it's co mpletely not uncommon for me to see\", metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content=\"someone using some machine learning algorith m and then explain to me what they've \\nbeen doing for the last six months, and I go, oh, gee, it should have been obvious from \\nthe start that the last six months, you've been wa sting your time, right?  \\nAnd so my goal in this class, running th rough the entire quarter , not just on learning \\ntheory, is actually not only to give you the tools of m achine learning, but to teach you\", metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content=\"how to use them well. And I've noticed this  is something that really not many other\", metadata={'source': 'test1.pdf', 'page': 14}),\n",
       " Document(page_content=\"classes teach. And this is something I'm rea lly convinced is a huge deal, and so by the \\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\nreally good at applying these learning algor ithms and getting them to work amazingly \\nwell in many problems. Okay?  \\nLet's see. So [inaudible] the board. After lear ning theory, there's a nother class of learning \\nalgorithms that I then want to teach you a bout, and that's unsupervised learning. So you\", metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content='recall, right, a little ea rlier I drew an example like this , right, where you have a couple of \\nfeatures, a couple of input vari ables and sort of malignant tumors and benign tumors or \\nwhatever. And that was an example of a s upervised learning problem because the data \\nyou have gives you the right answer for each of your patients. The data tells you this \\npatient has a malignant tumor;  this patient has a benign tumor. So it had the right', metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content='answers, and you wanted the algorithm to just produce more of the same.  \\nIn contrast, in an unsupervised learning problem , this is the sort of data you get, okay? \\nWhere speaking loosely, you\\'re given a data se t, and I\\'m not gonna tell you what the right \\nanswer is on any of your data. I\\'m just gonna give you a data set and I\\'m gonna say, \"Would you please find interesting structure in this data set?\" So that\\'s the unsupervised', metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content=\"learning problem where you're sort of not given the right answer for everything.  \\nSo, for example, an algorithm may find structure in the data in the form of the data being \\npartitioned into two clusters, or clustering is  sort of one example of an unsupervised \\nlearning problem.  \\nSo I hope you can see this. It turns out that th ese sort of unsupervised  learning algorithms \\nare also used in many problems. This is a scr een shot — this is a picture I got from Sue\", metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content=\"Emvee, who's a PhD student here, who is a pplying unsupervised learning algorithms to \\ntry to understand gene data, so is trying to  look at genes as individuals and group them \\ninto clusters based on properties of what ge nes they respond to — based on properties of \\nhow the genes respond to different experiments.  \\nAnother interesting application of [inaudible] sorts of clus tering algorithms is actually\", metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content=\"image processing, this which I got from Steve Gules, who's another PhD student. It turns \\nout what you can do is if you give this sort of  data, say an image, to certain unsupervised \\nlearning algorithms, they will then learn to group pixels together and say, gee, this sort of \\npixel seems to belong together , and that sort of pixel seems to belong together.  \\nAnd so the images you see on the bottom — I guess you can just barely see them on there\", metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content='— so the images you see on the bottom are groupings — are what the algorithm has done \\nto group certain pixels together. On a small di splay, it might be easier to just look at the \\nimage on the right. The two images on the botto m are two sort of identical visualizations \\nof the same grouping of the pixe ls into [inaudible] regions.  \\nAnd so it turns out that this sort of clustering algorithm or this sort of unsupervised', metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content='learning algorithm, which learns  to group pixels together, it turns out to be useful for \\nmany applications in vision, in co mputer vision image processing.', metadata={'source': 'test1.pdf', 'page': 15}),\n",
       " Document(page_content=\"I'll just show you one example, and this is a rather cool one that two students, Ashutosh \\nSaxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together,\", metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content=\"grouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single\", metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content=\"image and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different\", metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content='regions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look \\naround the campus. Okay? This actually turned out to be a mix of supervised and \\nunsupervised learning, but the unsupervised lear ning, this sort of cl ustering was the first \\nstep.  \\nSo it turns out these sorts of unsupervised — clustering algorithms are actually routinely', metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content=\"used for many different problems, things like organizing computing clusters, social \\nnetwork analysis, market segmentation, so if you're a marketer and you want to divide your market into different segments or diffe rent groups of people to market to them \\nseparately; even for astronomical data an alysis and understanding how galaxies are \\nformed. These are just a sort of small sample  of the applications of unsupervised learning\", metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content=\"algorithms and clustering algorithms that we 'll talk about later in this class.  \\nJust one particularly cool example of an uns upervised learning algorithm that I want to \\ntell you about. And to motivate that, I'm gonna  tell you about what's called the cocktail \\nparty problem, which is imagine that you're at  some cocktail party a nd there are lots of \\npeople standing all over. And you know how it is, right, if you're at a large party,\", metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content=\"everyone's talking, it can be sometimes very hard  to hear even the person in front of you. \\nSo imagine a large cocktail party with lots of people. So the problem is, is that all of these people talking, can you separate out the voice of just the person you're interested in \\ntalking to with all this  loud background noise?  \\nSo I'll show you a specific example in a second, but here's a cocktail party that's I guess\", metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content=\"rather sparsely attended by just two people.  But what we're gonna do is we'll put two \\nmicrophones in the room, okay? And so becau se the microphones are just at slightly \\ndifferent distances to the two people, and th e two people may speak in slightly different \\nvolumes, each microphone will pick up an overl apping combination of these two people's\", metadata={'source': 'test1.pdf', 'page': 16}),\n",
       " Document(page_content=\"voices, so slightly different overlapping voice s. So Speaker 1's voice may be more loud \\non Microphone 1, and Speaker 2's voice may be louder on Microphone 2, whatever.  \\nBut the question is, given these microphone reco rdings, can you separate out the original \\nspeaker's voices? So I'm gonna play some audi o clips that were collected by Tai Yuan \\nLee at UCSD. I'm gonna actually play for you the original raw microphone recordings \\nfrom this cocktail party. So this is the Microphone 1:\", metadata={'source': 'test1.pdf', 'page': 17}),\n",
       " Document(page_content=\"Microphone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\nUno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng) : So it's a fascinating cocktail party with people counting from \\none to ten. This is the second microphone:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\nUno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.\", metadata={'source': 'test1.pdf', 'page': 17}),\n",
       " Document(page_content=\"Instructor (Andrew Ng) : Okay. So in supervised learning, we don't know what the \\nright answer is, right? So what we're goi ng to do is take exactly the two microphone \\nrecordings you just heard and give it to an  unsupervised learning algorithm and tell the \\nalgorithm which of these discover structure in the data [inaudible] or  what structure is \\nthere in this data? And we actually don't know what the right answer is offhand.\", metadata={'source': 'test1.pdf', 'page': 17}),\n",
       " Document(page_content=\"So give this data to an unsupervised lear ning algorithm, and what the algorithm does in \\nthis case, it will discover that this data can actually be explained by two independent \\nspeakers speaking at the same time, and it can  further separate out the two speakers for \\nyou. So here's Output 1 of the algorithm:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nInstructor (Andrew Ng) : And there's the second algorithm:  \\nMicrophone 2:\", metadata={'source': 'test1.pdf', 'page': 17}),\n",
       " Document(page_content=\"Uno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng): And so the algorithm discovers  that, gee, the structure \\nunderlying the data is really th at there are two sources of so und, and here they are. I'll \\nshow you one more example. This is a, well, th is is a second sort of different pair of \\nmicrophone recordings:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:\", metadata={'source': 'test1.pdf', 'page': 18}),\n",
       " Document(page_content=\"Microphone 2:  \\n[Music playing.]  Instructor (Andrew Ng): So the poor guy is not at a cocktail party. He's talking to his \\nradio. There's the second recording:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng) : Right. And we get this data. It's the same unsupervised \\nlearning algorithm. The algorithm is actually called independent component analysis, and\", metadata={'source': 'test1.pdf', 'page': 18}),\n",
       " Document(page_content=\"later in this quarter, you'll see why. And then output's the following:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nInstructor (Andrew Ng): And that's the second one:  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng): Okay. So it turns out that be yond solving the cocktail party \\nalgorithm, this specific cla ss of unsupervised learning algor ithms are also applied to a\", metadata={'source': 'test1.pdf', 'page': 18}),\n",
       " Document(page_content=\"bunch of other problems, like in text proces sing or understanding f unctional grading and \\nmachine data, like the magneto-encephalogram would be an EEG data. We'll talk about \\nthat more when we go and describe ICA or independent component analysis algorithms, \\nwhich is what you just saw.\", metadata={'source': 'test1.pdf', 'page': 18}),\n",
       " Document(page_content=\"And as an aside, this algorithm I just showed you, it seems like it must be a pretty \\ncomplicated algorithm, right, to take this overlapping audio streams and separate them \\nout. It sounds like a pretty complicated thi ng to do. So you're gonna ask how complicated \\nis it really to implement an  algorithm like this? It turns out if you do it in MATLAB, you \\ncan do it in one line of code.  \\nSo I got this from Samuel Wyse at Toront o, U of Toronto, and the example I showed you\", metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content=\"actually used a more complicated ICA algorithm than this. But nonetheless, I guess this is \\nwhy for this class I'm going to ask you to  do most of your programming in MATLAB and \\nOctave because if you try to implement the sa me algorithm in C or Java or something, I \\ncan tell you from personal, painful experien ce, you end up writing pages and pages of \\ncode rather than relatively few lines of code. I'll also mention that it did take researchers\", metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content=\"many, many years to come up with that one line of code, so this is not easy.  \\nSo that was unsupervised learning, and then the last of the four major topics I wanna tell \\nyou about is reinforcement learning. And this  refers to problems where you don't do one-\\nshot decision-making. So, for example, in  the supervised learning cancer prediction \\nproblem, you have a patient come in, you predict that the cancer is malignant or benign.\", metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content=\"And then based on your prediction, maybe the pa tient lives or dies, and then that's it, \\nright? So you make a decision and then there's a consequence. You either got it right or \\nwrong. In reinforcement learning problems, you are usually asked to make a sequence of \\ndecisions over time.  \\nSo, for example, this is something that my students and I work on. If I give you the keys \\nto an autonomous helicopter — we actually ha ve this helicopter here at Stanford, — how\", metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content=\"do you write a program to make it fly, ri ght? You notice that if you make a wrong \\ndecision on a helicopter, the consequence of crashing it may not happen until much later. And in fact, usually you need to make a w hole sequence of bad decisions to crash a \\nhelicopter. But conversely, you al so need to make a whole sequence of good decisions in \\norder to fly a helic opter really well.  \\nSo I'm gonna show you some fun videos of lear ning algorithms flying helicopters. This is\", metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content=\"a video of our helicopter at Stanford flying using a contro ller that was learned using a \\nreinforcement learning algorithm. So this wa s done on the Stanford football field, and \\nwe'll zoom out the camera in a second. You'll sort of see th e trees planted in the sky. So \\nmaybe this is one of the most difficult aer obatic maneuvers flown on any helicopter under \\ncomputer control. And this controller, which is very, very hard for a human to sit down\", metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content='and write out, was learned using one of these reinforcement learning algorithms.  \\nJust a word about that: The basic idea behi nd a reinforcement learning algorithm is this \\nidea of what\\'s called a reward  function. What we have to think about is imagine you\\'re \\ntrying to train a dog. So every time y our dog does something good, you say, \"Good dog,\"', metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content='and you reward the dog. Every time your dog does something bad, you go, \"Bad dog,\" right? And hopefully, over time, your dog will lear n to do the right things to get more of \\nthe positive rewards, to get mo re of the \"Good dogs\" and to ge t fewer of the \"Bad dogs.”', metadata={'source': 'test1.pdf', 'page': 19}),\n",
       " Document(page_content='So the way we teach a helicopter to fly or any of these robots is sort of the same thing. \\nEvery time the helicopter crashes, we go, \"B ad helicopter,\" and every time it does the \\nright thing, we go, \"Good helicopter, \" and over time it learns how to control itself so as to \\nget more of these positive rewards.  \\nSo reinforcement learning is — I think of it as a way for you to specify what you want \\ndone, so you have to specify what is a \"good dog\" and what is a \"bad dog\" behavior. And', metadata={'source': 'test1.pdf', 'page': 20}),\n",
       " Document(page_content='then it\\'s up to the learning algorithm to  figure out how to maximize the \"good dog\" \\nreward signals and minimize the \"bad dog\" punishments.  \\nSo it turns out reinforcement learning is applie d to other problems in robotics. It\\'s applied \\nto things in web crawling and so on. But it\\'s just  cool to show videos, so let me just show', metadata={'source': 'test1.pdf', 'page': 20}),\n",
       " Document(page_content=\"a bunch of them. This learning algorithm was actually implemented by our head TA, Zico, of programming a four-legged dog. I guess Sam Shriver in this class also worked \\non the project and Peter Renfrew and Mike and a few others. But I guess this really is a \\ngood dog/bad dog since it's a robot dog.  \\nThe second video on the right, some of the st udents, I guess Peter, Zico, Tonca working \\non a robotic snake, again using learning algorith ms to teach a snake robot to climb over \\nobstacles.\", metadata={'source': 'test1.pdf', 'page': 20}),\n",
       " Document(page_content=\"obstacles.  \\nBelow that, this is kind of a fun example.  Ashutosh Saxena and Jeff Michaels used \\nlearning algorithms to teach a car how to  drive at reasonably high speeds off roads \\navoiding obstacles.  \\nAnd on the lower right, that's a robot program med by PhD student Eva Roshen to teach a \\nsort of somewhat strangely configured robot how to get on top of an obstacle, how to get \\nover an obstacle. Sorry. I know the video's kind of small. I hope you can sort of see it. \\nOkay?\", metadata={'source': 'test1.pdf', 'page': 20}),\n",
       " Document(page_content=\"Okay?  \\nSo I think all of these are robots that I thi nk are very difficult to hand-code a controller \\nfor by learning these sorts of l earning algorithms. You can in relatively short order get a \\nrobot to do often pretty amazing things.  \\nOkay. So that was most of what I wanted to say today. Just a couple more last things, but \\nlet me just check what questions you have righ t now. So if there are no questions, I'll just\", metadata={'source': 'test1.pdf', 'page': 20}),\n",
       " Document(page_content='close with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.', metadata={'source': 'test1.pdf', 'page': 20}),\n",
       " Document(page_content=\"So thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.\", metadata={'source': 'test1.pdf', 'page': 20}),\n",
       " Document(page_content='[End of Audio]  \\nDuration: 69 minutes', metadata={'source': 'test1.pdf', 'page': 21}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\nModule Title: Machine Learning ©UNIVERSITY OF LEEDS\\nSchool of Computing Semester 2 Practice Version\\nCalculator instructions:\\n- You areallowed to use a non-programmable calculator only from the following list of\\napproved models in this examination: Casio FX-82 ,Casio FX-83 ,Casio FX-85 .\\nDictionary instructions:\\n- You are notallowed to use your own dictionary in this examination. A basic English', metadata={'source': 'test2.pdf', 'page': 0}),\n",
       " Document(page_content='dictionary is available to use: raise your hand and ask an invigilator, if you need it.\\nExamination Information\\n- There are 9pages to this examination.\\n- There are 2 hours to complete the examination.\\n- Answer all 5 questions.\\n- The number in brackets [ ]indicates the marks available for each question or part\\nquestion.\\n- You are reminded of the need for clear presentation in your answers.\\n- The total number of marks for this examination paper is 66.\\n- You are allowed to use annotated materials.', metadata={'source': 'test2.pdf', 'page': 0}),\n",
       " Document(page_content='with solutions\\nPage 1of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 0}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\nQuestion 1\\nHMRC has been seizing illegal wine imports and collated statistics in Table 1, where wines are\\nsplit out by type and nationality.\\n(a) Consider the statistics in Table 1 on illegal wine imports. According to the table what are\\nthe marginal probabilities of seizing a French wine for white, rose and red? Give results\\nprecise to 2 decimals.\\nSolution:\\nThe three probabilities are given by:\\n0.05\\n0.05 + 0 .22 + 0 .17=0.05\\n0.44= 0.11\\nand0.22\\n0.44= 0.5\\nand0.17', metadata={'source': 'test2.pdf', 'page': 1}),\n",
       " Document(page_content='0.44= 0.5\\nand0.17\\n0.44= 0.39\\n[6 marks]\\n(b) What is probability of seizing a red wine, conditioned on it being Italian?\\nSolution:\\nThe probability of a wine being Italian is 0.56. Therefore the probability of wine being red,\\nconditioned on it being Italian is\\n0.23\\n0.56= 0.41\\n:\\n[4 marks]\\n[Question 1 Total: 10 marks]\\nwhite rose red\\nFrench 0.05 0.22 0.17\\nItalian 0.18 0.15 0.23\\nTable 1: HMRC has compiled statistics on fraudulent wine imports. The table represents joint', metadata={'source': 'test2.pdf', 'page': 1}),\n",
       " Document(page_content='probabilities on seized contraband.\\nPage 2of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 1}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\nQuestion 2\\nA test for melanoma detects 95 % of all true melanoma cases. When applied to people who don’t\\nhave melanoma, 4 % of the test results is positive. The prevalence of melanoma in the general\\npopulation is 0.1 %.\\n(a) Use Bayes’ law to estimate what the probability is that a person has melanoma upon\\nreceiving a positive test result.\\nSolution:\\nBayes’ law states:\\np(+|D) =P(+|D)P(D)\\nP(+|D)P(D) +P(+|̸D)P(̸D)', metadata={'source': 'test2.pdf', 'page': 2}),\n",
       " Document(page_content='Here Dsignifies the subject actually having melanoma and ̸Dthe subject not having\\nmelanoma. P(+|D) = 0 .5andP(+|̸D) = 0 .04.P(̸D) = 1−P(D).\\nSubstitution gives:\\n0.95·0.001\\n0.95·0.001 + 0 .04·0.999≈0.23\\n[6 marks]\\n[Question 2 Total: 6 marks]\\nPage 3of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 2}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\nQuestion 3\\nThe cross entropy is defined as: Ep[logq]. Assume a state space with three states and probabilities\\np1= 0.05, p2= 0.45, p3= 0.5.\\n(a) Calculate the cross-entropy between pandqfor the two following distributions of q:q1=\\n0.01, q2= 0.48, q3= 0.51andq1= 0.001, q2= 0.48, q3= 0.519.\\nSolution:\\nUsing the definition gives: 0.8972169147173776 and 1.0035995906433213\\n[4 marks]', metadata={'source': 'test2.pdf', 'page': 3}),\n",
       " Document(page_content='[4 marks]\\n(b) Comment on whether you expect to see a similar difference between the two qdistributions\\nif you would have used the mean squared error (it should not be necessary to perform the\\nactual calculation)? Explain why a measure that is sensitive to the difference between the\\ntwoqdistributions can be important for some problems.\\nSolution:\\nThe mean squared error would deliver a small difference between both cases. But the', metadata={'source': 'test2.pdf', 'page': 3}),\n",
       " Document(page_content='frequency of event 1 happening is a factor of 10 higher for the second q-distribution, which\\nis much harder to detect from the MSE than from the cross-entropy.\\n[4 marks]\\n[Question 3 Total: 8 marks]\\nPage 4of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 3}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\nFigure 1: A dataset consisting of points that have been atrributed to two different classes: one\\nindicated by ’+’ and one by ’x’.\\nPage 5of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 4}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\nQuestion 4\\nIn Figure 1 you see a dataset with points being labeled into two classes. One class is indicated by\\n’+’, the other by ’x’. You want to be able to classify new data points using logistic regression.\\nSomeone has drawn a line by eye in the dataset with the aim of separating the classes. The line\\npasses the points (6,-2) and (-2,6). When we mention the term ’weights’ in this question, this\\nincludes all parameters, also an intercept when one is needed.', metadata={'source': 'test2.pdf', 'page': 5}),\n",
       " Document(page_content='(a) Give the mathematical definition of a logistic regression classifier for this two-dimensional\\ndataset, and explain how to interpret its output and how you can use that to decide\\nwhich class a novel point belongs to. Provide an explicit formula in terms of the xandy\\ncoordinates of a point.\\nSolution:\\nA logistic classsifier in this case is defined by:\\no=1\\n1 +e−(w0+w1x+w2y)\\nThe use of the minus sign is unimportant. Its output is a number between 0 and 1, which is', metadata={'source': 'test2.pdf', 'page': 5}),\n",
       " Document(page_content='typically interpreted as the probability of a data point to one of the two classes. A decision\\nis usually made based on whether the probability is smaller or larger than a half.\\n[4 marks]\\n(b) Give a logistic classifier that is based on the line drawn in the figure. Any set of weights is\\nacceptable, as long as the line represents the set of points where a point has exactly 50 %\\nprobability of falling into the ’+’ class.\\nSolution:', metadata={'source': 'test2.pdf', 'page': 5}),\n",
       " Document(page_content='Solution:\\nThe line passing through the points (-2, 6), (6,-2) is x+y= 4, sox+y−4 = 0 is the\\nline of 50 % probability. This means e.g. w0=−4, w1= 1, w2= 1.\\n[6 marks]\\n(c) Present a geometrical argument for the statement that a loss function for logistic regression\\nhas a global minimum. You are allowed to base your argument on Figure 1 and to generalise\\nit. Explain why the existence of a global minimum simplifies finding a minimum for the\\ncost function.\\nSolution:', metadata={'source': 'test2.pdf', 'page': 5}),\n",
       " Document(page_content='Solution:\\nIt is clear that the optimal line for separation must be close to the one drawn because any\\nother line would give many more misclassifications. It is clear that depending on the loss\\nfunction, one line will be the best, so there is a global maximum. In a higher dimensional\\nspace the argument remains the same. [4 marks]. This helps with techniques such as\\nsteepest gradient descent because there are no local minima where the technique can get\\nstuck.\\n[6 marks]', metadata={'source': 'test2.pdf', 'page': 5}),\n",
       " Document(page_content='stuck.\\n[6 marks]\\nPage 6of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 5}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\n(d) On a much larger dataset, someone has estimated means and covariance for both classes.\\nThe results are as follows:\\nµ’+’= (3,4)T, µ’x’= (1,1)T\\nand\\nΣ’+’=\\x122 0\\n0 2\\x13\\n= Σ’x’.\\nBased on this information, give the equation for the 50 % probability line. Explain whether\\nthe line drawn in the figure is accurate or not.\\nSolution:\\nStudents have been given a formula for calculating this. However, they may conclude on', metadata={'source': 'test2.pdf', 'page': 6}),\n",
       " Document(page_content='the basis of a geometrical argument that the line equidistant between the points (1,1) and\\n(3,4) gives the right answer. This line is perpendicular to the direction (2,3) and must\\npass through the halfway point between them. So 2x+ 3y=c, forcsuch that it passes\\n(2,2.5), which gives c= 11.5. In standard form this is y=−2/3x+3.83. [7] marks for the\\ncalculation. [3] marks for concluding that the line is not quite correct because its gradient\\nis off.\\n[10 marks]\\n[Question 4 Total: 26 marks]', metadata={'source': 'test2.pdf', 'page': 6}),\n",
       " Document(page_content='Page 7of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 6}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\nSample No. Regularity of tumour boundary Tumour Diameter Classification\\nSample 1 Regular Large Benign\\nSample 2 Irregular Small Malignant\\nSample 3 Irregular Small Benign\\nSample 4 Regular Small Benign\\nSample 5 Irregular Large Malignant\\nSample 6 Irregular Large Malignant\\nSample 7 Regular Small Benign\\nSample 8 Regular Large Benign\\nSample 9 Regular Small Benign\\nSample 10 Irregular Small Malignant', metadata={'source': 'test2.pdf', 'page': 7}),\n",
       " Document(page_content='Table 2: A small dataset of tumour attributes and their classification as health risk.\\nQuestion 5\\nIt is well known that tumours with regular boundaries are often benign and those with irregular\\nboundaries are often malignant. However, it is also fairly common to find that malignant tumours\\ntend to be bigger and have larger diameters than benign tumours. Given the data set (of N =\\n10 samples) described in Table 2, one can find the optimal order for splitting attributes/features', metadata={'source': 'test2.pdf', 'page': 7}),\n",
       " Document(page_content='(i.e. regularity of boundary and tumour diameter) in a binary decision tree for classifying these\\ndata into two classes: Malignant tumour vs Benign tumour.\\n(a) Do this for the ID3 algorithm. Support your answer by calculating the information gain for\\nthe chosen order of splits. Show your workings.\\nSolution:\\nID3:\\nThe entropy of a set S is given by H(S) =−P\\nipilog2(pi)\\nP(B) = 0 .6, P(M) = 0 .4, denote set of samples (S).\\nEntropy of full set:\\nH(S) =−0.6∗log2(0.6)–0.4∗log2(0.4) = 0 .9710', metadata={'source': 'test2.pdf', 'page': 7}),\n",
       " Document(page_content='H(S|Attribute 1) = 0 .5∗(0) + 0 .5∗(−1/5∗log2(1/5)−4/5∗log2(4/5)) = 0 .3609\\nInformation Gain: IG(Attribute 1) = H(S)–H(S|Attribute 1) = 0 .9710−0.3609 =\\n0.6101\\nH(S|Attribute 2) = 0 .6∗(−4/6∗log2(4/6)–2/6∗log2(2/6))+\\n0.4∗(−2/4∗log2(2/4)–2/4∗log2(2/4)) = 0 .9510\\nInformation Gain: IG(Attribute 2) = H(S)–H(S|Attribute 2) = 0 .9710–0 .9510 =\\n0.0200 .\\nTherefore, split first on Attribute 1 (regularity of tumour boundary) as information gain is\\nhigher.\\n[8 marks]\\nPage 8of9 Turn the page over', metadata={'source': 'test2.pdf', 'page': 7}),\n",
       " Document(page_content='SolutionsModule Code: COMP561101\\n(b) Do the same for the CART algorithm based on the GINI gain. Show your workings.\\nSolution:\\nCART:\\nThe Gini impurity of a set S is given by G(S) =P\\nipi(1−pi)\\nGini impurity of full set:\\nG(S) = 0 .6∗(1−0.6) + 0 .4∗(1−0.4) = 0 .48\\nG(S|Attribute 1) = 0 .5∗(0) + 0 .5∗(1/5∗(1−1/5) + 4 /5∗(1−4/5)) = 0 .16\\nGini Gain: GG(Attribute 1) = G(S)–G(S|Attribute 1) = 0 .48\\nH(S|Attribute 2) = 0 .6∗(4/6∗(1−4/6) + 2 /6∗(1−2/6))+\\n0.4∗(2/4∗(1−2/4) + 2 /4∗(1−2/4)) = 0 .4667', metadata={'source': 'test2.pdf', 'page': 8}),\n",
       " Document(page_content='Gini Gain: GG(Attribute 2) = G(S)–G(S|Attribute 2) = 0 .48–0.4667 = 0 .0133\\nTherefore, split first on Attribute 1 (regularity of tumour boundary) as resulting Gini gain\\nis higher.\\n[8 marks]\\n[Question 5 Total: 16 marks]\\n[Grand Total: 66 marks]\\nPage 9of9 End', metadata={'source': 'test2.pdf', 'page': 8}),\n",
       " Document(page_content='1 \\n \\n Abstract  1 \\nRelation extraction stands as a pivotal task 2 \\nin natural language processing, where 3 \\nadeptly capturing semantic relationships 4 \\nbetween entities holds paramount 5 \\nimportance. Among the crucial features in 6 \\nrelation extraction tasks, entity information 7 \\nstands out prominently. However, 8 \\nprevailing neural network models have yet 9 \\nto fully exploit entity information . As joint 10 \\nentity and relation extraction is one of the 11', metadata={'source': 'test3.pdf', 'page': 0}),\n",
       " Document(page_content='most important facet in Natural language 12 \\nprocessing tasks, even so a few existing had 13 \\nfocused on considering possible relational 14 \\ninformation between entities before 15 \\nextracting them, leading the models to not 16 \\nconstitute valid triplets. To address this gap, 17 \\nwe propose two models. The first model 18 \\namalgamates multi -head attention 19 \\nmechanisms  with Bi -LSTM  This 20 \\namalgamation effectively harnesses 21 \\ncontextual information from the input 22', metadata={'source': 'test3.pdf', 'page': 0}),\n",
       " Document(page_content=\"sequence, dynamically directing attention 23 \\nto segments pertinent to the relation 24 \\nextraction task.  The second model uses  25 \\nSyntax -Induced pre -training with 26 \\ndependency masking to improve upon  27 \\nHeterogeneous graph neural network for 28 \\nrelation extraction . Our experiments on the 29 \\nSemEval -2010 Task 8 benchmark dataset 30 \\nshowcase the performance of our model. It 31 \\nenhances the model's generalization 32 \\ncapability and training efficiency to a 33\", metadata={'source': 'test3.pdf', 'page': 0}),\n",
       " Document(page_content='notable degree, facilitating more accurate 34 \\npredictions when handling text sequences 35 \\nwith intricate structures.  36 1 Introduction  37 \\nIn the field of Natural Language Processing (NLP), 38 \\nrelation extraction stands as a critical task aimed at 39 \\nidentifying and extracting semantic relationships 40 \\nbetween entities from textual data. This task holds 41 \\nsignificant importance for various NLP 42 \\napplications, including information retrieval, 43', metadata={'source': 'test3.pdf', 'page': 0}),\n",
       " Document(page_content='question answering systems, and knowledge graph 44 \\nconstruction  (Nguyen and Grishman, 2015) . The 45 \\nrelation classification task is defined as predicting 46 \\nthe semantic relationship between two annotated 47 \\nentities in a sentence  (Hendrickx et al., 2019) .  48 \\nSentence  \\nFinancial <e1> stress </e1> is one of the main causes \\nof <e2> divorce </e2>  \\nEntity 1:     stress  Entity 2:    divorce  \\nRelation  \\nCause -Effect(e1,e2)  \\nTable 1: A example of relation extraction  49', metadata={'source': 'test3.pdf', 'page': 0}),\n",
       " Document(page_content='Consider the sentence provided in Sentence 50 \\nTable 1 as an illustration. The entities within the 51 \\nsentence have been labeled accordingly, with 52 \\n<e1></e1>  enclosing the first entity and 53 \\n<e2></e2>  enclosing the second entity. In this 54 \\ninstance, the relationship between these entities is 55 \\nclassified as Cause -Effect(e1,e2) . 56 \\nAccurate relationship classification is crucial 57 \\nfor precise sentence interpretation, discourse 58', metadata={'source': 'test3.pdf', 'page': 0}),\n",
       " Document(page_content='processing, and higher -level tasks in translation 59 \\nNLP (Hendrickx et al., 2010). Consequently, the 60 \\ntask of relation extraction has garnered significant 61 \\nattention over the past decades (Qian et al., 2009; 62 \\nRink and Harabagiu, 2010). Supervised methods, 63 \\nmeticulously crafted from lexical and semantic 64  \\n \\n \\n \\nExploring Graph Neural Networks, Bidirectional LSTMs, \\nand BERT for Enhanced Relation Extraction  \\n \\nUniversity of Manchester  \\n \\n \\nYuhang Wu     Neel More     Ye Liu', metadata={'source': 'test3.pdf', 'page': 0}),\n",
       " Document(page_content='2 \\n \\n resources, have achieved remarkable performance 65 \\n(Hendrickx, 2010; Rink and Harabagiu, 2010). 66 \\nHowever, effectively integrating feature selection 67 \\nand knowledge sources remains a challenging 68 \\naspect of relation classification.  69 \\nIn previous research, deep neural networks 70 \\nhave been widely applied to relation extraction 71 \\ntasks, exhibiting the capability to derive effective 72 \\nfeatures from both lexical and sentence levels (CN 73', metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content='Santos, 2015; Zhang, 2015; Zeng et al., 2014; Yu et 74 \\nal., 2014).  75 \\n       Recently due to the introduction of BERT 76 \\n(Bidirectional Encoder Representations from 77 \\nTransformers), which requires a single layer on top 78 \\nof the pre -trained bidirectional representations 79 \\nmany state -of-the art models can be built with ease. 80 \\nAllowing us to achieve cutting -edge performances 81 \\nand fostering innovation in the domain of language 82 \\nprocessing (Jacob et al., 2019) . 83', metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content='Building upon prior work, we introduce two 84 \\nnovel approaches. The first method is rooted in 85 \\ntraditional neural network models, where we 86 \\namalgamate multi -head attention mechanisms with 87 \\nBi-LSTM. Furthermore, we incorporate Dropout 88 \\nregularization techniques and Xavier initialization 89 \\nmethods. This results in the Multi -Attention Bi - 90 \\nLSTM model , which achieves enhanced accuracy 91 \\nwithout relying on additional knowledge or NLP 92', metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content=\"systems, thereby effectively bolstering the model's 93 \\ngeneralization capability and training efficiency.  94 \\nIn the second model, to learn a better text encoder 95 \\nthat has important structural knowledge about the 96 \\nsentence context the model employs the method of 97 \\ntraining the encoder with masking meanwhile 98 \\nrecovering word to word dependency and word 99 \\nconnections that are analyzed from the parsers. The 100 \\nparser thus can be considered as weekly supervised 101\", metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content='but sufficiently provided with the syntax 102 \\nintegration (Tian et al., 2022).  103 \\nThe significance of dependency parsers can 104 \\nclearly be observed in the demonstrations of many 105 \\nof the previous works where LSTM combined with 106 \\nshort dependency path proves valuable in relation 107 \\nclassification. With this knowledge we have tried 108 \\nto embed the encoder with syntax knowledge (Yan 109 \\nXu et .al, 2015).  The remainder of the paper is 110', metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content='structured as follows.  Section 3 elaborates on the 111 \\nstructure of the Multihead Attention BiLSTM 112 \\nmodel, while Section 4 introduces the architecture 113 \\nof SIP-RIFRE model. Section 5 covers our dataset, 114 \\nexperimental setup, as well as experimental results 115 and discussions. Finally, Section 6 provides a 116 \\nsummary of our paper.  117 \\n2 Related Work  118 \\nNumerous studies have investigated relation 119 \\nclassification, employing various methodologies. 120', metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content='Early approaches predominantly entailed manual 121 \\nfeature engineering using NLP tools or handcrafted 122 \\nkernels (Lee, 2019). For example, Rink and 123 \\nHarabagiu (2010) proposed utilizing a Support 124 \\nVector Machine (SVM) model alongside external 125 \\ncorpus features for relation extraction.  126 \\nAs neural networks advanced, they became 127 \\nprevalent in relation extraction tasks. Zeng et al. 128 \\n(2014) pioneered the use of Convolutional Neural 129', metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content=\"Networks (CNNs) for relation classification, 130 \\nachieving an accuracy of 82.7% by capturing local 131 \\ntext features through convolutional operations. 132 \\nSubsequently, dos Santos et al. (2015) enhanced 133 \\nthis approach with the CR -CNN model, which 134 \\nleverages ranking methods to capture entity 135 \\nrelations. Introducing attention mechanisms, 136 \\nHuang and Shen (2016) devised the Attention CNN 137 \\nmodel, augmenting the model's focus on salient 138\", metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content='information and achieving an accuracy of 84.3%. 139 \\nWang et al. (2016) further advanced this field with 140 \\nthe Multi -Attention CNN model, attaining an 141 \\naccuracy of 88.0%.  142 \\nAdditionally, Recurrent Neural Networks 143 \\n(RNNs) have seen widespread adoption in relation 144 \\nextraction. Zhang et al. (2015) proposed 145 \\nBidirectional Long Short -Term Memory Networks 146 \\n(Bi-LSTM) for relation classification, achieving an 147 \\naccuracy of 82.7%. Xiao and Liu (2016), as well as 148', metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content=\"Zhou et al. (2016), introduced the Hierarchical 149 \\nAttention Bi -LSTM and Attention Bi -LSTM 150 \\nmodels, respectively, incorporating attention 151 \\nmechanisms and achieving accuracies of 84.3% 152 \\nand 84.0%. More recently, Lee et al. (2019) 153 \\nintroduced the Entity Attention Bi -LSTM model, 154 \\nwhich refines the model's focus on entity 155 \\ninformation using entity -aware attention 156 \\nmechanisms, yielding an accuracy of 85.2%. Our 157\", metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content=\"primary model leverages RNNs, integrating multi - 158 \\nhead attention mechanisms and Bi -LSTM, thereby 159 \\nenriching the model's comprehension of 160 \\ninformation across diverse semantic levels through 161 \\nmulti -level attention mechanisms.  162 \\nThe previous studies in the field of 163 \\nBidirectional encoder representation have 164 \\nshowcased the importance of pre -training the 165 \\nmodel with the same training data but on different 166\", metadata={'source': 'test3.pdf', 'page': 1}),\n",
       " Document(page_content='3 \\n \\n directions to achieve a fine -tuning scheme, and 167 \\nhyper -parameters for BERT (Vaswani A et .al, 168 \\n2017). The studies in Graph networks especially 169 \\ngraph attention networks with masking had shown 170 \\nextraordinary performances when incorporated 171 \\nwith masking and self -attention mechanisms 172 \\nallowing to assign different weights to different 173 \\nnodes within the same neighborhood, at the same 174 \\ntime dealing with a large neighborhood. These 175', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='graph networks deals with the node and edges 176 \\nrepresentation of data does not seem to require the 177 \\nentire graph structure upfront, thus allows to 178 \\nexpand from a small isolated context space 179 \\ngradually. These graph networks does not rely on 180 \\ncomplex and intensive matrix operations and are 181 \\nhereby more computationally efficient. This study 182 \\ncan be seen in the work done by (Petar V et .al, 183 \\n2018).  184 \\n3 Multi -Attention Bi -LSTM Model  185', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='3.1 Word Embeddings  186 \\nLet a input sentence is denoted by  𝑆={𝑥1,𝑥2,…, 187 \\n𝑥𝑇}, where T is the number of words , we transform 188 \\neach word  𝑥𝑇  into a ve ctor representations  𝑒𝑖  by 189 \\nlooking up word embedding matrix 𝑊𝑤𝑜𝑟𝑑∈ 190 \\n ℝ𝑑𝑤|𝑉|, where 𝑑𝑤 is the dimension of the vector, 191 \\n|𝑉|  is a fixed -sized vocabulary, 𝑊𝑤𝑜𝑟𝑑   is a 192 \\nparameter needs to be learned. We use matrix - 193 \\nvector product to get word embedding 𝑒𝑖, then the 194', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content=\"word representations 𝑒𝑚𝑏𝑠={𝑒1,𝑒2,…,𝑒𝑇}, are 195 \\nobtained, and fed into next layers.  196 \\nWe used the pre -trained word embedding 197 \\nmodel from GloVe , and to  prevent overfitting, a 198 \\nDropout layer is added after the word embedding 199 \\nlayer. This layer randomly drops some elements of 200 \\nthe word embedding vectors, reducing the model's 201 \\nreliance on the training data excessively and 202 \\nenhancing its generalization ability.  203 \\n3.2 Bidirectional LSTM  204\", metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='The Long Short -Term Memory (LSTM) unit was 205 \\nintroduced by Hochreiter and Schmidhuber (1997) 206 \\nto address the vanishing gradient problem. 207 \\nSubsequently, numerous LSTM variants have 208 \\nemerged. We employ a variant proposed by Graves 209 \\net al. (2013), which incorporates a weighted 210 \\nglimpse connection carousel (CEC) from constant 211 \\nerror to gates within the same memory block. This 212 \\nmechanism enables the direct generation of gate 213', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='current cell states using the current cell state 214 \\n(Graves, 2013).  215 The following is the LSTM formula for a 216 \\nsingle memory block , where  𝑖𝑡 is the Input Gates , 217 \\n𝑓𝑡  is the Forget Gates , 𝑐𝑡  is Cells , 𝑜𝑡  is Output 218 \\nGates , ℎ𝑡  is Cell Outputs , and 𝜎  is the activation 219 \\nfunction :  220 \\n𝑖𝑡=𝜎(𝑊𝑥𝑖𝑥𝑡+𝑊ℎ𝑖ℎ𝑡−1+𝑊𝑐𝑖𝑐𝑡−1+𝑏𝑖 ) 221 \\n𝑓𝑡=𝜎(𝑊𝑥𝑓𝑥𝑡+𝑊ℎ𝑓ℎ𝑡−1+𝑊𝑐𝑓𝑐𝑡−1+𝑏𝑓 ) 222 \\n𝑐𝑡=𝑓𝑡𝑐𝑡−1+tanh(𝑊𝑥𝑐𝑥𝑡+𝑊ℎ𝑐ℎ𝑡−1+𝑏𝑐) 223 \\n𝑜𝑡=𝜎(𝑊𝑥𝑜𝑥𝑡+𝑊ℎ𝑜ℎ𝑡−1+𝑊𝑐𝑜𝑐𝑡+𝑏𝑜 ) 224 \\n ℎ𝑡=𝑜𝑡tanh(𝑐𝑡) 225', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='ℎ𝑡=𝑜𝑡tanh(𝑐𝑡) 225 \\nIn this paper, we use Bi -LSTM, The network 226 \\ncomprises two subnetworks dedicated to left - 227 \\nsequence context and right -sequence context, 228 \\nrespectively, facilitating forward and backward 229 \\npropagation.  We use the following equation to 230 \\ncombine the forward and backward pass outputs : 231 \\nℎ𝑖=[ ℎ𝑖⃗⃗⃗ ⊕ℎ𝑖⃖⃗⃗⃗ ] 232 \\n3.3 Multi -head Attention  233 \\nThe multi -head attention mechanism is a powerful 234 \\nneural network structure commonly employed for 235', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='processing sequential data. It enables simultaneous 236 \\nattention over different parts of the input sequence 237 \\nand learns meaningful representations from them  238 \\n(V oita et al., 2019) . In our model, we utilize a multi - 239 \\nhead self -attention mechanism, structured  in figure 240 \\n1.  241 \\nWe input a sequence 𝑋={𝑥1,𝑥2,…,𝑥𝑛}  and 242 \\nutilize linear transformations to map X to d - 243 \\ndimensional query (Q), key (K), and value (V) 244', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='spaces, yielding Q, K, and V . For each attention 245 \\nhead i, we compute the attention weights ai for each 246 \\nhead and perform a weighted sum over the 247 \\ncorresponding sequence values V , yielding the 248 \\noutput for each head as follows:  249 \\nα𝑖=softmax(𝑄𝑊𝑖𝑄(𝐾𝑊𝑖𝐾)𝑇\\n√𝑑𝑘) 250 \\nThen we concatenate the outputs of h attention 251 \\nheads together to form the output of the attention 252 \\nmechanism.  253 \\nIn our model, the multi -head attention 254', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='mechanism takes the output vector matrix H from 255 \\nthe LSTM layer as input and computes a weighted 256 \\nsum to obtain the representation r of the sentence 257 \\npair, the equation as following:  258 \\n𝑟=𝐻α𝑇 259', metadata={'source': 'test3.pdf', 'page': 2}),\n",
       " Document(page_content='4 \\n \\n Finally, through a tanh activation function, we 260 \\nobtain the final representation ℎ ∗ for classification 261 \\nof the sentence pair.  262 \\nℎ∗=tanh(𝑟) 263 \\nThis representation preserves crucial 264 \\ninformation from the sentence pair and can be 265 \\nutilized for subsequent relation classification tasks.  266 \\n3.4 Classifying and Training  267 \\nWe employ a SoftMax  classifier to predict the label 268 \\n𝑦̂for sentence S, where the labels are drawn from a 269', metadata={'source': 'test3.pdf', 'page': 3}),\n",
       " Document(page_content='discrete set of classes Y . The classifier takes the 270 \\nhidden state as input, and the loss function is the 271 \\nnegative log -likelihood of the true class labels 𝑦̂ , 272 \\nand the conditional probability 𝑝(𝑦|𝑆;𝜃)  is as 273 \\nfollowing:  274 \\n𝑝(𝑦|𝑆;θ)=softmax(𝑊𝑂𝑧+𝑏𝑂) 275 \\nWe utilize dropout  for regularization , a 276 \\ntechnique introduced by Hinton et al.  (2012 ). 277 \\nDropout is applied to the embedding layer, LSTM 278 \\nlayer, and the penultimate layer. Additionally, we 279', metadata={'source': 'test3.pdf', 'page': 3}),\n",
       " Document(page_content='rescale the weight vectors w to have a L2 norm of 280 \\n‖𝑤‖=𝑠 after each gradient descent step if ‖𝑤‖> 281 \\n𝑠.  282 \\n4 Second Model  283 \\n4.1 Word Embedding  284 \\nThe word embedding used in the SIP -RIFRE 285 \\nmodel relies on order dependencies. Dependency 286 parser is applied onto the input to obtain the 287 \\nunderlying tree structure Tx . The input sentences 288 \\ncan be represented  𝑋={𝑥1,𝑥2,…,𝑥𝑇} 289 \\nY * M=DM(DE(Tx)) 290 \\nThe approach extracts first, second and third 291', metadata={'source': 'test3.pdf', 'page': 3}),\n",
       " Document(page_content='order dependency Tx and represents it in the form 292 \\nof a tuple ( 𝑥i , 𝑥j , type). Where there is a relation 293 \\nbetween 𝑥i , 𝑥j  and type is directional. For first 294 \\norder dependency a direct connection from Tx , 295 \\nhowever for second order dependency a connection 296 \\nis made between 𝑥i  and 𝑥j  if there exist another 297 \\nword that connects both 𝑥i and𝑥j  with the 298 \\nconnections ( 𝑥i  , 𝑥0  ) and (𝑥0  , 𝑥j ) in Tx, 299 \\nThree types for their connections namely; ancestor, 300', metadata={'source': 'test3.pdf', 'page': 3}),\n",
       " Document(page_content='sister, and descendant are defined according to the 301 \\nposition of xi and xj in the dependency tree TX. 302 \\nSimilarly for third order other dependencies are 303 \\ndefined, ancestor, uncle, nephew, and descendant .  304 \\n 305 \\n 306 \\nFigure 2: The left part shows the process to extract and 307 \\nmask dependencies (connection and type masking, 308 \\nrespectively) in first, second, and third orders, where the 309 \\nword subscript denotes its sentential index. The right 310', metadata={'source': 'test3.pdf', 'page': 3}),\n",
       " Document(page_content='part illustrates the process to compute the scores of 311 \\ndependency connections and types in different orders to 312 \\nrecover the ones that being masked ( Tian, Y et .al, 2022) . 313 \\n5 Experiments  314 \\n5.1 Data set 315 \\nOur model is evaluated on the SemEval -2010 Task 316 \\n8 dataset, comprising 10 distinct relations: Cause - 317 \\nEffect, Instrument -Agency, Product -Producer, 318 \\nContent -Container, Entity -Origin, Entity - 319 \\nDestination, Component -Whole, Member - 320', metadata={'source': 'test3.pdf', 'page': 3}),\n",
       " Document(page_content='Collection, Message -Topic, and Other. The first 321 \\nnine relations are bidirectional, while Other is non - 322 \\ndirectional, resulting in a total of 19 relations. The 323 \\ndataset consists of 10,717 annotated sentences, 324 \\nwith 8,000 samples allocated for training and 2,717 325 \\n \\nFigure 1: Multi -Head Self Attention  (Lee et al. , \\n2019) .', metadata={'source': 'test3.pdf', 'page': 3}),\n",
       " Document(page_content='5 \\n \\n samples for testing. We utilize the official 326 \\nevaluation metric of SemEval -2010 Task 8, which 327 \\nis based on the macro -averaged F1 -score.  328 \\n5.2 Implementation Details  329 \\nThe hyperparameters utilized during the training 330 \\nprocess for our proposed two models are as follows:  331 \\nHyper -\\nParameters  Value Description  \\nword_dim  100 Word  Embedding  size  \\nepoch  50 Number of epoch  \\nbatch_size  10 Mini_Batch size \\nlr 1.0 Learning Rate  \\ndropout  0.3 Word Embedding layer', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='0.3 BLSTM layer  \\n0.5 Entity -aware Attention \\nlayer  \\nlayers_num  1 Number of LSTM  \\nL2_decay  1e-05 L2 Regularization \\nCoefficient  \\n     Table 2: Hperpar meters used for Multi-BiLSTM  332 \\nHyper -\\nParameters  Value Description  \\nhidden_size 768 Hidden dimension  \\nepoch  50 Number of epoch  \\nvocab_ size 10 V ocabulary file  \\nlr 1.0 Learning Rate  \\nintermediate_ size 3072 Intermediate size  \\nmax_posit ion_em  512 Maximum position \\nembeddings  \\nTable 3: Hyperparamters used in SIP -RIFRE  333', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='Architecture  Accuracy  \\nMulti -Attention CNN \\n(Wang et al. 2016)  88.0 \\nEntity Attention Bi -\\nLSTM (Lee et al., \\n2019)  85.2 \\nAttention Bi -LSTM \\n(Zhou et al., 2016)  84.0 \\nREDN  91 \\nRELA  90.6 \\nSP  91.9 Our models   \\nSIP-RIFRE  91.3 \\nMult -Att BiLSTM  83.82  \\nTable 3: Experimental Results  334 \\nThe table above displays various architectures used 335 \\nfor relation extraction and the corresponding 336 \\naccuracy scores produced on the SemEval -2010 337 \\nTask-8 Dataset.  338', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='The Multi attention CNNs implementation 339 \\nshown above combines the strengths of CNNs in 340 \\ncapturing local features and attention mechanisms 341 \\nin focusing on relevant parts of the text to identify 342 \\nrelationships between entities however adding 343 \\nmultiple heads contributes to model complexity.  344 \\nCNNs generally acquire noise over the period and 345 \\nare also not very adept al learning long term 346 \\ndependencies contributing to the poor performance 347', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='compared to other Bidirectional Graph neural 348 \\nnetwork models.  349 \\nLong Short Term Memory has been used 350 \\nsince a long time and had seen a lot of variations in 351 \\nNatural Language Processing.  Bidirectional  LSTM 352 \\nmodels also tend to become computationally 353 \\ncomplex at some point if we try to make it retain 354 \\nmost of the information, therefore it is better to use 355 \\nmodels with lea rned weights rather than relying on 356', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='large amounts of data which LSTMs benefit from.  357 \\n 358 \\nGraph networks have proved to be less 359 \\ncomputationally extensive and are able to capture 360 \\nrelational information just as efficiently as CNNs 361 \\nbut require the text to be transformed into a graph 362 \\nstructure. However GNNs are sensitive to noisy 363 \\ndata thus focuses mostly on structured data.  364 \\nIntroduction of BERT has revolutionized the 365 \\nNLP domain and has still many other applications 366', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='to be explored. Our BERT based model uses 367 \\nheterogeneous graph structures that encode 368 \\nrelations and words in different nodes allowing us 369 \\nto explore the contexts more efficiently , our other 370 \\nmodel combines Multi attention with Bidirectional 371 \\nLSTMs which helps the model to cover more 372 \\ntextual information and explore and learn 373 \\nembedding in all directions.  374 \\nConclusion  375 \\nDuring our exploration of different models we have 376 \\ncome to a few conclusions:  377', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='Relation extraction has witnessed significant 378 \\nadvancements with various architectures offering 379', metadata={'source': 'test3.pdf', 'page': 4}),\n",
       " Document(page_content='6 \\n \\n unique approaches. Multi -attention CNNs and 380 \\nBidirectional LSTMs have mainly steadied the 381 \\nfield for a while the emergence of BERT has 382 \\nopened new avenues for relation extraction, 383 \\nparticularly with it’s ability to handle 384 \\nheterogeneous graphs. Overall the choice of model 385 \\ndepends on factors like data availability, 386 \\ncomputation resources, and desired level of 387 \\ninterpretability.  388 \\nReferences  389 \\n[1] Hendrickx, I., Kim, S.N., Kozareva, Z., 390', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='Nakov, P., Sé aghdha, D.O., Padó , S., 391 \\nPennacchiotti, M., Romano, L. and 392 \\nSzpakowicz, S., 2019. Semeval -2010 task 8: 393 \\nMulti -way classification of semantic 394 \\nrelations between pairs of nominals. arXiv 395 \\npreprint arXiv:1911.10422.  396 \\n[2] Lee, J., Seo, S. and Choi, Y.S., 2019. 397 \\nSemantic relation classification via 398 \\nbidirectional lstm networks with entity - 399 \\naware attention using latent entity typing. 400 \\nSymmetry, 11(6), p.785.  401', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='[3] Nguyen, T.H. and Grishman, R., 2015, June. 402 \\nRelation extraction: Perspective from 403 \\nconvolutional neural networks. In 404 \\nProceedings of the 1st workshop on vector 405 \\nspace modeling for natural language 406 \\nprocessing (pp. 39 -48). 407 \\n[4] Qian, L., Zhou, G., Kong, F. and Zhu, Q., 408 \\n2009, August. Semi -supervised learning for 409 \\nsemantic relation classification using 410 \\nstratified sampling strategy. In Proceedings 411 \\nof the 2009 conference on empirical methods 412', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='in natural language processing (pp. 1437 - 413 \\n1445).  414 \\n[5] Rink, B. and Harabagiu, S., 2010, July. Utd: 415 \\nClassifying semantic relations by combining 416 \\nlexical and semantic resources. In 417 \\nProceedings of the 5th international 418 \\nworkshop on semantic evaluation (pp. 256 - 419 \\n259).  420 \\n[6] Santos, C.N.D., Xiang, B. and Zhou, B., 421 \\n2015. Classifying relations by ranking with 422 \\nconvolutional neural networks. arXiv 423 \\npreprint arXiv:1504.06580.  424', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='[7] Yu, M., Gormley, M. and Dredze, M., 2014, 425 \\nDecember. Factor -based compositional 426 \\nembedding models. In NIPS workshop on 427 \\nlearning semantics (pp. 95 -101).  428 \\n[8] Zeng, D., Liu, K., Lai, S., Zhou, G. and Zhao, 429 \\nJ., 2014, August. Relation classification via 430 \\nconvolutional deep neural network. In 431 \\nProceedings of COLING 2014, the 25th 432 international conference on computational 433 \\nlinguistics: technical papers (pp. 2335 -2344).  434', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='[9] Zhang, S., Zheng, D., Hu, X. and Yang, M., 435 \\n2015, October. Bidirectional long short -term 436 \\nmemory networks for relation classification. 437 \\nIn Proceedings of the 29th Pacific Asia 438 \\nconference on language, information and 439 \\ncomputation (pp. 73 -78). 440 \\n[10] Graves, A., 2013. Generating sequences with 441 \\nrecurrent neural networks. arXiv preprint 442 \\narXiv:1308.0850.  443 \\n[11] Hochreiter, S. and Schmidhuber, J., 1997. 444 \\nLong short -term memory. Neural 445', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='computation, 9(8), pp.1735 -1780.  446 \\n[12] Lee, J., Seo, S. and Choi, Y.S., 2019. 447 \\nSemantic relation classification via 448 \\nbidirectional lstm networks with entity - 449 \\naware attention using latent entity typing. 450 \\nSymmetry, 11(6), p.785.  451 \\n[13] Santos, C.N.D., Xiang, B. and Zhou, B., 452 \\n2015. Classifying relations by ranking with 453 \\nconvolutional neural networks. arXiv 454 \\npreprint arXiv:1504.06580.  455 \\n[14] Shen, Y. and Huang, X.J., 2016, December. 456', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='Attention -based convolutional neural 457 \\nnetwork for semantic relation extraction. In 458 \\nProceedings of COLING 2016, the 26th 459 \\nInternational Conference on Computational 460 \\nLinguistics: Technical Papers (pp. 2526 - 461 \\n2536).  462 \\n[15] Voita, E., Talbot, D., Moiseev, F., Sennrich, 463 \\nR. and Titov, I., 2019. Analyzing multi -head 464 \\nself-attention: Specialized heads do the 465 \\nheavy lifting, the rest can be pruned. arXiv 466 \\npreprint arXiv:1905.09418.  467', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='[16] Wang, L., Cao, Z., De Melo, G. and Liu, Z., 468 \\n2016, August. Relation classification via 469 \\nmulti -level attention cnns. In Proceedings of 470 \\nthe 54th Annual Meeting of the Association 471 \\nfor Computational Linguistics (Volume 1: 472 \\nLong Papers) (pp. 1298 -1307).  473 \\n[17] Xiao, M. and Liu, C., 2016, December. 474 \\nSemantic relation classification via 475 \\nhierarchical recurrent neural network with 476 \\nattention. In Proceedings of COLING 2016, 477', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='the 26th International Conference on 478 \\nComputational Linguistics: Technical 479 \\nPapers (pp. 1254 -1263).  480 \\n[18] Zeng, D., Liu, K., Lai, S., Zhou, G. and Zhao, 481 \\nJ., 2014, August. Relation classification via 482 \\nconvolutional deep neural network. In 483 \\nProceedings of COLING 2014, the 25th 484 \\ninternational conference on computational 485 \\nlinguistics: technical papers (pp. 2335 -2344).  486', metadata={'source': 'test3.pdf', 'page': 5}),\n",
       " Document(page_content='7 \\n \\n [19] Zhang, S., Zheng, D., Hu, X. and Yang, M., 487 \\n2015, October. Bidirectional long short -term 488 \\nmemory networks for relation classification. 489 \\nIn Proceedings of the 29th Pacific Asia 490 \\nconference on language, information and 491 \\ncomputation (pp. 73 -78). 492 \\n[20] Zhou, P., Shi, W., Tian, J., Qi, Z., Li, B., Hao, 493 \\nH. and Xu, B., 2016, August. Attention - 494 \\nbased bidirectional long short -term memory 495 \\nnetworks for relation classification. In 496', metadata={'source': 'test3.pdf', 'page': 6}),\n",
       " Document(page_content='Proceedings of the 54th annual meeting of 497 \\nthe association for computational linguistics 498 \\n(volume 2: Short papers) (pp. 207 -212).  499 \\n[21] Devlin, J., Chang, M.W., Lee, K. and 500 \\nToutanova, K., 2018. Bert: Pre -training of 501 \\ndeep bidirectional transformers for language 502 \\nunderstanding.  arXiv preprint 503 \\narXiv:1810.04805 . 504 \\n[22] Zhao, K., Xu, H., Cheng, Y., Li, X. and Gao, 505 \\nK., 2021. Representation iterative fusion 506 \\nbased on heterogeneous graph neural 507', metadata={'source': 'test3.pdf', 'page': 6}),\n",
       " Document(page_content='network for joint entity and relation 508 \\nextraction.  Knowledge -Based Systems , 219, 509 \\np.106888.  510 \\n[23] Tian, Y., Song, Y. and Xia, F., 2022, May. 511 \\nImproving relation extraction through 512 \\nsyntax -induced pre -training with 513 \\ndependency masking. In  Findings of the 514 \\nAssociation for Computational Linguistics: 515 \\nACL 2022  (pp. 1875 -1886).  516 \\n[24] Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H. 517 \\nand Jin, Z., 2015, September. Classifying 518', metadata={'source': 'test3.pdf', 'page': 6}),\n",
       " Document(page_content='relations via long short term memory 519 \\nnetworks along shortest dependency paths. 520 \\nIn Proceedings of the 2015 conference on 521 \\nempirical methods in natural language 522 \\nprocessing  (pp. 1785 -1794).  523 \\n[25] Yan, H., Qiu , X. and Huang, X., 2020. A 524 \\ngraph -based model for joint chinese word 525 \\nsegmentation and dependency 526 \\nparsing.  Transactions of the Association for 527 \\nComputational Linguistics , 8, pp.78 -92. 528 \\n 529 \\n 530 \\n 531 \\n 532 \\n 533 \\n 534', metadata={'source': 'test3.pdf', 'page': 6}),\n",
       " Document(page_content='Exercise sheet: Decision trees and ensemble methods\\nThe following exercises have different levels of difficulty indicated by (*), (**), (***). An exercise with (*)\\nis a simple exercise requiring less time to solve compared to an exercise with (***), which is a more complex\\nexercise.\\n1. (**)[ Entropy ] We are given a deck of ncards in order 1 ,2, ..., n . Then a randomly chosen card is\\nremoved and placed at a random position in the deck. What is the entropy of the resulting deck of\\ncard?', metadata={'source': 'test4.pdf', 'page': 0}),\n",
       " Document(page_content='card?\\n2. (**)[ Entropy ] Does there exist a discrete random variable Xwith a distribution such that H(X) =\\n+∞? If so, describe it as explicitly as possible.\\n3. (*) [ Decision Trees ]The table below lists a sample of data from a census. There are four descrip-\\nID AGE EDUCATION MARITAL STATUS OCCUPATION ANNUAL INCOME\\n1 39 bachelors never married transport 25K-50K\\n2 50 bachelors married professional 25K-50K\\n3 18 high school never married agriculture ≤25K\\n4 28 bachelors married professional 25K-50K', metadata={'source': 'test4.pdf', 'page': 0}),\n",
       " Document(page_content='5 37 high school married agriculture 25K-50K\\n6 24 high school never married armed forces ≤25K\\n7 52 high school divorced transport 25K-50K\\n8 40 doctorate married professional ≥50K\\ntive features and one target feature in this dataset: AGE, EDUCATION, MARITAL STATUS and\\nOCCUPATION. The target feature is the ANNUAL INCOME.\\n(a) Calculate information gain (based on entropy) for the EDUCATION, MARITAL STATUS,\\nand OCCUPATION features.', metadata={'source': 'test4.pdf', 'page': 0}),\n",
       " Document(page_content='(b) Calculate information gain using the Gini index for the EDUCATION, MARITAL STATUS,\\nand OCCUPATION features.\\n(c) When building a decision tree, the easiest way to handle a continuous feature is to define a\\nthreshold around which splits will be made. What would be the optimal threshold to split the\\ncontinuous AGE feature (use information gain based on entropy as the feature selection measure)?', metadata={'source': 'test4.pdf', 'page': 0}),\n",
       " Document(page_content='4. (*) [ Decision Trees ] The following table lists a dataset of the scores students achieved on an exam\\ndescribed in terms of whether the student studied for the exam (STUDIED) and the energy level of the\\nlecturer when grading the student’s exam (ENERGY). Which of the two descriptive features should\\nwe use as the testing criterion at the root node of a decision tree to predict students’ scores?', metadata={'source': 'test4.pdf', 'page': 0}),\n",
       " Document(page_content='5. (**) [ Ensemble Methods ] The following table lists a dataset containing the details of five participants\\nin a heart disease study, and a target feature RISK which describes their risk of heart disease. Each\\n1', metadata={'source': 'test4.pdf', 'page': 0}),\n",
       " Document(page_content='ID STUDIED ENERGY SCORE\\n1 yes tired 65\\n2 no alert 20\\n3 yes alert 90\\n4 yes tired 70\\n5 no tired 40\\n6 yes alert 85\\n7 no tired 35\\npatient is described in terms of four descriptive features: EXERCISE (how regularly do they exercise?),\\nSMOKER (do they smoke?), OBESE (are they overweight?) FAMILY (did any of their parents or\\nsiblings suffer from heart disease?).\\nID EXERCISE SMOKER OBESE FAMILY RISK\\n1 daily false false yes low\\n2 weekly true false yes high\\n3 daily false false no low', metadata={'source': 'test4.pdf', 'page': 1}),\n",
       " Document(page_content='4 rarely true true yes high\\n5 rarely true true no high\\n(a) As part of the study researchers have decided to create a predictive model to screen participants\\nbased on their risk of heart disease. You have been asked to implement this screening model using\\narandom forest . The three tables below list three bootstrap samples that have been generated\\nfrom the above dataset. Using these bootstrap samples create the decision trees that will be in', metadata={'source': 'test4.pdf', 'page': 1}),\n",
       " Document(page_content='the random forest model (use entropy based information gain as the feature selection criterion).\\nID EXERCISE FAMILY RISK\\n1 daily yes low\\n2 weekly yes high\\n2 weekly yes high\\n5 rarely no high\\n5 rarely no high\\nBoostrap Sample AID SMOKER OBESE RISK\\n1 false false low\\n2 true false high\\n2 true false high\\n4 true true high\\n5 true true high\\nBoostrap Sample BID OBESE FAMILY RISK\\n1 false yes low\\n1 false yes low\\n2 false yes high\\n4 true yes high\\n5 true no high\\nBoostrap Sample C', metadata={'source': 'test4.pdf', 'page': 1}),\n",
       " Document(page_content='Boostrap Sample C\\n(b) Assuming the random forest model you have created uses majority voting, what prediction will\\nit return for the following query:\\nEXERCISE=rarely, SMOKER=false, OBESE=true, FAMILY=yes.\\n2', metadata={'source': 'test4.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "409288b4-3e0d-40f3-bffb-4ca8a4548c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6606353-6a8a-4fe8-857f-f5a98d70ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee101145-e88a-4aba-8131-664db0239919",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(api_key = \"sk-proj-B4fsYjOG4RM3LT15ig1hT3BlbkFJoTg40k62UTmoIP24MqeK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f228c82-8708-45d4-8787-ccaf0fb8440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I like PeiMa\"\n",
    "sentence2 = \"PeiMa like me\"\n",
    "sentence3 = \"We like each other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1a92e1c-378b-4350-8991-ebf56edf9209",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding1 \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m embedding2 \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_query(sentence2)\n\u001b[0;32m      3\u001b[0m embedding3 \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_query(sentence3)\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:697\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:668\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:494\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    492\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 494\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    500\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:116\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\resources\\embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anything\\Software\\MiniConda\\envs\\langchain\\lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f93bd-ba41-491c-bc93-437b3e110a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
